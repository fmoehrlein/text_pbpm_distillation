{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88958776",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562c959f",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d137acf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Installing missing packages: ['sentence_transformers']\n",
      "Looking in indexes: https://nexus.iisys.de/repository/ki-awz-pypi-group/simple, https://pypi.org/simple\n",
      "Requirement already satisfied: sentence_transformers in /opt/conda/lib/python3.12/site-packages (5.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.12/site-packages (from sentence_transformers) (4.56.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from sentence_transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.12/site-packages (from sentence_transformers) (2.8.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.12/site-packages (from sentence_transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.12/site-packages (from sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.12/site-packages (from sentence_transformers) (0.34.5)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.12/site-packages (from sentence_transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/conda/lib/python3.12/site-packages (from sentence_transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2025.9.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/conda/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2025.9.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (1.1.10)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence_transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence_transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence_transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence_transformers) (2025.8.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn->sentence_transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn->sentence_transformers) (3.6.0)\n",
      "Updating the git repository...\n",
      "Already up to date.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "\n",
    "def install_missing_requirements(requirements_path):\n",
    "    if not os.path.exists(requirements_path):\n",
    "        print(f\"Requirements file '{requirements_path}' not found.\")\n",
    "        return\n",
    "\n",
    "    with open(requirements_path) as f:\n",
    "        required = [line.strip() for line in f if line.strip() and not line.startswith(\"#\")]\n",
    "\n",
    "    installed = {pkg.key for pkg in pkg_resources.working_set}\n",
    "    \n",
    "    missing = []\n",
    "    for req in required:\n",
    "        pkg_name = req.split(\"==\")[0].lower()\n",
    "        if pkg_name not in installed:\n",
    "            missing.append(req)\n",
    "\n",
    "    if not missing:\n",
    "        print(\"All required packages are already installed.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Installing missing packages: {missing}\")\n",
    "\n",
    "    # Check if running inside conda\n",
    "    conda_prefix = os.environ.get(\"CONDA_PREFIX\")\n",
    "    if conda_prefix:\n",
    "        print(\"Detected conda environment. Trying to use conda first...\")\n",
    "        for pkg in missing:\n",
    "            pkg_name = pkg.split(\"==\")[0]\n",
    "            try:\n",
    "                subprocess.check_call([\"conda\", \"install\", \"-y\", pkg_name])\n",
    "            except subprocess.CalledProcessError:\n",
    "                print(f\"Package '{pkg_name}' not found in conda. Falling back to pip.\")\n",
    "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
    "    else:\n",
    "        # Not in conda, use pip directly\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", *missing])\n",
    "\n",
    "\n",
    "def update_repository():\n",
    "    print(\"Updating the git repository...\")\n",
    "    try:\n",
    "        result = subprocess.run([\"git\", \"pull\"], capture_output=True, text=True, check=True)\n",
    "        print(result.stdout)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"Git pull failed:\")\n",
    "        print(e.stderr)\n",
    "\n",
    "# install missing requirements\n",
    "install_missing_requirements(\"/home/jovyan/requirements_reddit.txt\")\n",
    "\n",
    "# set cwd to the project root\n",
    "cwd = os.getcwd()\n",
    "wd = '/home/jovyan/reddit-mining/'\n",
    "if cwd != wd:\n",
    "    os.chdir(wd)\n",
    "\n",
    "# update the git repository\n",
    "update_repository()\n",
    "\n",
    "# set PYTHONPATH to the src directory\n",
    "sys.path.append('src')\n",
    "\n",
    "# set up logging\n",
    "from logger import setup_logger\n",
    "#logger = setup_logger(level=logging.INFO)\n",
    "logger = setup_logger(level=logging.DEBUG)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cc85a5",
   "metadata": {},
   "source": [
    "## Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dd3184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating the git repository...\n",
      "Already up to date.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cddf1f190574c72bdb93690e540e99d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "parsing log, completed traces ::   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "update_repository()\n",
    "# from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "from tapp.tapp_model import _get_event_labels, TappModel\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "\n",
    "folder_name = \"mimicel\"\n",
    "\n",
    "if folder_name == \"werk\":\n",
    "    path = \"./data/werk.xes\"\n",
    "    data_attributes = [\"age\"]\n",
    "    text_attribute = \"question\"\n",
    "elif folder_name == \"mimicel\":\n",
    "    path = \"./data/mimicel_mini_2000.xes\"\n",
    "    data_attributes = [\"case:acuity\"]\n",
    "    text_attribute = \"case:chiefcomplaint\"\n",
    "\n",
    "variant = xes_importer.Variants.ITERPARSE\n",
    "parameters = {\n",
    "    variant.value.Parameters.TIMESTAMP_SORT: True,\n",
    "    variant.value.Parameters.REVERSE_SORT: False,\n",
    "}\n",
    "log = xes_importer.apply(path, variant=variant, parameters=parameters)\n",
    "activities = _get_event_labels(log, \"concept:name\")\n",
    "class_names = _get_event_labels(log, \"concept:name\")\n",
    "class_names.append(\"END\")\n",
    "split = len(log) // 5 * 4\n",
    "train_log = log[:split]\n",
    "test_log = log[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf933f84",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f41dd45",
   "metadata": {},
   "source": [
    "## Shorten Event Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "460fb627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "072d21a379784ecc8dd4c2875d881ae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "exporting log, completed traces ::   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mini log saved at: ./data/mimicel_mini_20000_mini_2000.xes\n"
     ]
    }
   ],
   "source": [
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "from pm4py.objects.log.exporter.xes import exporter as xes_exporter\n",
    "from pm4py.objects.log.obj import EventLog  # <-- needed to wrap the sliced log\n",
    "\n",
    "max_cases = 2000\n",
    "\n",
    "mini_log = EventLog(log[:max_cases])  # wrap slice as EventLog\n",
    "\n",
    "# --- BUILD OUTPUT PATH ---\n",
    "folder, filename = os.path.split(path)\n",
    "name, ext = os.path.splitext(filename)\n",
    "out_path = os.path.join(folder, f\"{name}_mini_{len(mini_log)}{ext}\")\n",
    "\n",
    "# --- EXPORT MINI LOG ---\n",
    "xes_exporter.apply(mini_log, out_path)\n",
    "\n",
    "print(f\"Mini log saved at: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e07d40b",
   "metadata": {},
   "source": [
    "## Get Distilled Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4281bd92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating the git repository...\n",
      "Already up to date.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'case:acuity'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m text_model \u001b[38;5;129;01min\u001b[39;00m text_models:\n\u001b[32m     23\u001b[39m \n\u001b[32m     24\u001b[39m     \u001b[38;5;66;03m# initialize and fit the log encoder\u001b[39;00m\n\u001b[32m     25\u001b[39m     log_encoder = LogEncoder(\n\u001b[32m     26\u001b[39m         text_encoder=text_model,\n\u001b[32m     27\u001b[39m         advanced_time_attributes=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     28\u001b[39m         text_base_for_training=\u001b[33m\"\u001b[39m\u001b[33mevent\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     29\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[43mlog_encoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m        \u001b[49m\u001b[43mactivities\u001b[49m\u001b[43m=\u001b[49m\u001b[43mactivities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata_attributes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_attributes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_attribute\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_attribute\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m     X_train, y_train, _ = log_encoder.transform(train_log, for_training=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     38\u001b[39m     X_test, y_test, _ = log_encoder.transform(test_log, for_training=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/reddit-mining/src/tapp/log_encoder.py:47\u001b[39m, in \u001b[36mLogEncoder.fit\u001b[39m\u001b[34m(self, log, activities, data_attributes, text_attribute)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28mself\u001b[39m.data_attributes = data_attributes\n\u001b[32m     46\u001b[39m \u001b[38;5;28mself\u001b[39m.text_attribute = text_attribute\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m \u001b[38;5;28mself\u001b[39m.categorical_attributes = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mattribute\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_is_numerical_attribute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattribute\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata_attributes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[38;5;28mself\u001b[39m.categorical_attributes_values = [_get_event_labels(log, attribute) \u001b[38;5;28;01mfor\u001b[39;00m attribute \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.categorical_attributes]\n\u001b[32m     49\u001b[39m \u001b[38;5;28mself\u001b[39m.numerical_attributes = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m attribute: _is_numerical_attribute(log, attribute), \u001b[38;5;28mself\u001b[39m.data_attributes))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/reddit-mining/src/tapp/log_encoder.py:47\u001b[39m, in \u001b[36mLogEncoder.fit.<locals>.<lambda>\u001b[39m\u001b[34m(attribute)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28mself\u001b[39m.data_attributes = data_attributes\n\u001b[32m     46\u001b[39m \u001b[38;5;28mself\u001b[39m.text_attribute = text_attribute\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m \u001b[38;5;28mself\u001b[39m.categorical_attributes = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m attribute: \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43m_is_numerical_attribute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattribute\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m.data_attributes))\n\u001b[32m     48\u001b[39m \u001b[38;5;28mself\u001b[39m.categorical_attributes_values = [_get_event_labels(log, attribute) \u001b[38;5;28;01mfor\u001b[39;00m attribute \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.categorical_attributes]\n\u001b[32m     49\u001b[39m \u001b[38;5;28mself\u001b[39m.numerical_attributes = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m attribute: _is_numerical_attribute(log, attribute), \u001b[38;5;28mself\u001b[39m.data_attributes))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/reddit-mining/src/tapp/log_encoder.py:330\u001b[39m, in \u001b[36m_is_numerical_attribute\u001b[39m\u001b[34m(log, attribute)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_is_numerical_attribute\u001b[39m(log, attribute):\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _is_numerical(\u001b[43mlog\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mattribute\u001b[49m\u001b[43m]\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/pm4py/objects/log/obj.py:100\u001b[39m, in \u001b[36mEvent.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[31mKeyError\u001b[39m: 'case:acuity'"
     ]
    }
   ],
   "source": [
    "update_repository()\n",
    "from tapp.log_encoder import LogEncoder\n",
    "from tapp.tapp_model import TappModel, _get_event_labels\n",
    "from tapp.text_encoder import BoWTextEncoder\n",
    "from tapp.text_encoder import BoNGTextEncoder\n",
    "from tapp.text_encoder import LDATextEncoder\n",
    "from tapp.text_encoder import BERTbaseTextEncoder\n",
    "from tapp.text_encoder import BERTbaseFineTunedNextActivityTextEncoder\n",
    "from distillation import get_distillation_paths\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "text_models = [\n",
    "    #None,\n",
    "    #BoWTextEncoder(encoding_length=50, language=\"english\"),\n",
    "    #BoNGTextEncoder(n=2, encoding_length=50, language=\"english\"),\n",
    "    #LDATextEncoder(encoding_length=100, language=\"english\"),\n",
    "    BERTbaseTextEncoder(encoding_length=768, language=\"english\"),\n",
    "    #BERTbaseFineTunedNextActivityTextEncoder(encoding_length=768, language=\"english\", epochs=16, lr=5e-5),\n",
    "]\n",
    "\n",
    "for text_model in text_models:\n",
    "\n",
    "    # initialize and fit the log encoder\n",
    "    log_encoder = LogEncoder(\n",
    "        text_encoder=text_model,\n",
    "        advanced_time_attributes=True,\n",
    "        text_base_for_training=\"event\",\n",
    "    )\n",
    "    log_encoder.fit(\n",
    "        log,\n",
    "        activities=activities,\n",
    "        data_attributes=data_attributes,\n",
    "        text_attribute=text_attribute,\n",
    "    )\n",
    "\n",
    "    X_train, y_train, _ = log_encoder.transform(train_log, for_training=True)\n",
    "    X_test, y_test, _ = log_encoder.transform(test_log, for_training=True)\n",
    "    y_train = y_train.argmax(axis=1)\n",
    "    y_test = y_test.argmax(axis=1)\n",
    "\n",
    "    # prepare distillation path\n",
    "    distillation_path_train, distillation_path_test = get_distillation_paths(folder_name, text_model)\n",
    "\n",
    "    # tappbert hyperparameters\n",
    "    shared_layer = 1\n",
    "    special_layer = 1\n",
    "    neuron = 100\n",
    "\n",
    "    # tappbert training and evaluation\n",
    "    print(\"Training and evaluating TAPPBERT model...\")\n",
    "    tapp_model = TappModel(\n",
    "        log_encoder=log_encoder,\n",
    "        num_shared_layer=shared_layer,\n",
    "        num_specialized_layer=special_layer,\n",
    "        neurons_per_layer=neuron,\n",
    "        dropout=0.2,\n",
    "        learning_rate=0.001,\n",
    "    )\n",
    "    tapp_model.activities = activities\n",
    "    tapp_model.fit(\n",
    "        train_log, data_attributes=data_attributes, text_attribute=text_attribute, epochs=25\n",
    "    )\n",
    "    y_train_distilled = tapp_model.model.predict(X_train)\n",
    "    y_train_distilled = y_train_distilled[0]\n",
    "    np.save(distillation_path_train, y_train_distilled)\n",
    "    print(\"Saved distilled training data to:\", distillation_path_train)\n",
    "    y_test_distilled = tapp_model.model.predict(X_test)\n",
    "    y_test_distilled = y_test_distilled[0]\n",
    "    np.save(distillation_path_test, y_test_distilled)\n",
    "    print(\"Saved distilled test data to:\", distillation_path_test)\n",
    "    tapp_model.evaluate(test_log, \"results.csv\", num_prefixes=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e090ec",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "700d71ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating the git repository...\n",
      "Already up to date.\n",
      "\n",
      "Using text models: []\n",
      "Training and evaluating for k=5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed training data shape: (44240, 125)\n",
      "Transformed test data shape: (10980, 125)\n",
      "(44240, 125) (44240,)\n",
      "DT (original): acc - 0.4708, f1 - 0.3974, con_acc - 0.8499, con_f1 - 0.8329\n",
      "Number of nodes: 253\n",
      "Max depth: 20\n",
      "(44240, 125) (44240,)\n",
      "DT (distilled): acc - 0.4606, f1 - 0.3879, con_acc - 0.9096, con_f1 - 0.8912\n",
      "Number of nodes: 457\n",
      "Max depth: 21\n",
      "Using text models: ['BoW']\n",
      "Training and evaluating for k=5...\n",
      "Transformed training data shape: (44240, 375)\n",
      "Transformed test data shape: (10980, 375)\n",
      "(44240, 375) (44240,)\n",
      "DT (original): acc - 0.4888, f1 - 0.4227, con_acc - 0.8885, con_f1 - 0.8847\n",
      "Number of nodes: 335\n",
      "Max depth: 22\n",
      "(44240, 375) (44240,)\n",
      "DT (distilled): acc - 0.4802, f1 - 0.4229, con_acc - 0.9714, con_f1 - 0.9711\n",
      "Number of nodes: 541\n",
      "Max depth: 26\n",
      "Using text models: ['BoNG']\n",
      "Training and evaluating for k=5...\n",
      "Transformed training data shape: (44240, 375)\n",
      "Transformed test data shape: (10980, 375)\n",
      "(44240, 375) (44240,)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 131\u001b[39m\n\u001b[32m    129\u001b[39m X_train_sampled, train_labels_sampled = resample_training_data(X_train, train_labels, method=sampling_method)\n\u001b[32m    130\u001b[39m \u001b[38;5;66;03m# Train + evaluate\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m acc, f1, con_acc, con_f1 = \u001b[43mevaluate_distillation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstudent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train_sampled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_labels_sampled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_test_distilled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdesc\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[38;5;66;03m# Save results\u001b[39;00m\n\u001b[32m    142\u001b[39m meta = \u001b[38;5;28mdict\u001b[39m(\n\u001b[32m    143\u001b[39m     description=desc,\n\u001b[32m    144\u001b[39m     model_type=model_type,\n\u001b[32m   (...)\u001b[39m\u001b[32m    153\u001b[39m     ccp=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    154\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/reddit-mining/src/distillation.py:47\u001b[39m, in \u001b[36mevaluate_distillation\u001b[39m\u001b[34m(model, X_train, X_test, y_train, y_test, y_test_distilled, description, output)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mevaluate_distillation\u001b[39m(\n\u001b[32m     45\u001b[39m     model, X_train, X_test, y_train, y_test, y_test_distilled, description=\u001b[33m\"\u001b[39m\u001b[33mEvaluation\u001b[39m\u001b[33m\"\u001b[39m, output=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     46\u001b[39m ):\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m     y_pred = model.predict(X_test)\n\u001b[32m     49\u001b[39m     acc = accuracy_score(y_test, y_pred)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/sklearn/tree/_classes.py:1024\u001b[39m, in \u001b[36mDecisionTreeClassifier.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, check_input)\u001b[39m\n\u001b[32m    993\u001b[39m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    994\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight=\u001b[38;5;28;01mNone\u001b[39;00m, check_input=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    995\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[32m    996\u001b[39m \n\u001b[32m    997\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1021\u001b[39m \u001b[33;03m        Fitted estimator.\u001b[39;00m\n\u001b[32m   1022\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1028\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1029\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1030\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/sklearn/tree/_classes.py:472\u001b[39m, in \u001b[36mBaseDecisionTree._fit\u001b[39m\u001b[34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    462\u001b[39m     builder = BestFirstTreeBuilder(\n\u001b[32m    463\u001b[39m         splitter,\n\u001b[32m    464\u001b[39m         min_samples_split,\n\u001b[32m   (...)\u001b[39m\u001b[32m    469\u001b[39m         \u001b[38;5;28mself\u001b[39m.min_impurity_decrease,\n\u001b[32m    470\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m \u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.n_outputs_ == \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    475\u001b[39m     \u001b[38;5;28mself\u001b[39m.n_classes_ = \u001b[38;5;28mself\u001b[39m.n_classes_[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "update_repository()\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tapp.text_encoder import BoWTextEncoder, BoNGTextEncoder, LDATextEncoder, BERTbaseTextEncoder\n",
    "from tapp.log_encoder import LogEncoder\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from distillation import (\n",
    "    get_distillation_paths,\n",
    "    get_evaluation_paths,\n",
    "    evaluate_distillation,\n",
    "    analyze_text_splits,\n",
    "    prepare_text_feature_datasets,\n",
    "    concatenate_text_feature_datasets,\n",
    "    get_feature_datasets,\n",
    "    save_evaluation_results,\n",
    "    tree_to_str,\n",
    ")\n",
    "import numpy as np\n",
    "import sys\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "# ----------------------------\n",
    "# Factory for student models\n",
    "# ----------------------------\n",
    "def get_student_model(model_type=\"dt\", random_state=0, **kwargs):\n",
    "    model_type = model_type.lower()\n",
    "    if model_type == \"dt\":\n",
    "        return DecisionTreeClassifier(random_state=random_state, **kwargs)\n",
    "    elif model_type == \"ebm\":\n",
    "        return ExplainableBoostingClassifier(random_state=random_state, **kwargs)\n",
    "    elif model_type == \"lasso\":\n",
    "        return LogisticRegression(\n",
    "            penalty=\"l1\",\n",
    "            solver=\"saga\",\n",
    "            max_iter=5000,\n",
    "            multi_class=\"multinomial\",\n",
    "            random_state=random_state,\n",
    "            **kwargs\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown student model type: {model_type}\")\n",
    "    \n",
    "def resample_training_data(X, y, method=\"smote\"):\n",
    "    if method == \"smote\":\n",
    "        sampler = SMOTE(sampling_strategy='not majority', random_state=42)\n",
    "    elif method == \"random\":\n",
    "        sampler = RandomOverSampler(sampling_strategy='not majority', random_state=42)\n",
    "    else:\n",
    "        return X, y\n",
    "    return sampler.fit_resample(X, y)\n",
    "\n",
    "# ----------------------------\n",
    "# Params\n",
    "# ----------------------------\n",
    "student_model_types = [\"dt\", \"ebm\", \"lasso\"]   # choose which students to try\n",
    "student_model_types = [\"dt\"]   # choose which students to try\n",
    "model_names = [\"BoW\", \"BoNG\", \"LDA\"]\n",
    "text_model_tapp = BERTbaseTextEncoder(encoding_length=768, language=\"english\")\n",
    "text_models = [\n",
    "    BoWTextEncoder(encoding_length=50, language=\"english\"),\n",
    "    BoNGTextEncoder(n=2, encoding_length=50, language=\"english\"),\n",
    "    LDATextEncoder(encoding_length=100, language=\"english\"),\n",
    "]\n",
    "k_values = [5]\n",
    "ccp_alpha = 0.0001\n",
    "sampling_method = None \n",
    "\n",
    "# ----------------------------\n",
    "# Load distilled labels\n",
    "# ----------------------------\n",
    "distillation_path_train, distillation_path_test = get_distillation_paths(folder_name, text_model_tapp)\n",
    "y_train_distilled = np.load(distillation_path_train)\n",
    "y_test_distilled = np.load(distillation_path_test)\n",
    "y_train_distilled = y_train_distilled.argmax(axis=1)\n",
    "y_test_distilled = y_test_distilled.argmax(axis=1)\n",
    "\n",
    "# ----------------------------\n",
    "# Evaluation loop\n",
    "# ----------------------------\n",
    "model_names_powerset = [list(subset) \n",
    "    for r in range(len(model_names)+1) \n",
    "    for subset in itertools.combinations(model_names, r)]\n",
    "\n",
    "for model_names_subset in model_names_powerset:\n",
    "    print(f\"Using text models: {model_names_subset}\")\n",
    "    for k in k_values:\n",
    "        print(f\"Training and evaluating for k={k}...\")\n",
    "        train_dataset, test_dataset = get_feature_datasets(\n",
    "            folder_name,\n",
    "            text_models,\n",
    "            log,\n",
    "            train_log,\n",
    "            test_log,\n",
    "            k=k,\n",
    "        )\n",
    "        y_train = train_dataset[\"y\"].argmax(axis=1)\n",
    "        y_test = test_dataset[\"y\"].argmax(axis=1)\n",
    "        X_train, features = concatenate_text_feature_datasets(train_dataset, model_names_subset)\n",
    "        X_test, _ = concatenate_text_feature_datasets(test_dataset, model_names_subset)\n",
    "        print(\"Transformed training data shape:\", X_train.shape)\n",
    "        print(\"Transformed test data shape:\", X_test.shape)\n",
    "\n",
    "        for model_type in student_model_types:\n",
    "                for version, train_labels in [\n",
    "                    (\"original\", y_train),\n",
    "                    (\"distilled\", y_train_distilled),\n",
    "                ]:\n",
    "                    desc = f\"{model_type.upper()} ({version})\"\n",
    "                    model_path, model_str_path, model_features_path, model_y_path = get_evaluation_paths(\n",
    "                        folder_name,\n",
    "                        version,\n",
    "                        model_type=model_type,\n",
    "                        model_names_subset=model_names_subset,\n",
    "                        k=k,\n",
    "                        ccp_alpha=ccp_alpha,\n",
    "                    )\n",
    "\n",
    "                    # Create student model\n",
    "                    if model_type == \"dt\":\n",
    "                        student = get_student_model(model_type, ccp_alpha=ccp_alpha)\n",
    "                    else:\n",
    "                        student = get_student_model(model_type)\n",
    "\n",
    "                    # sampling\n",
    "                    print(X_train.shape, train_labels.shape)\n",
    "                    X_train_sampled, train_labels_sampled = resample_training_data(X_train, train_labels, method=sampling_method)\n",
    "                    # Train + evaluate\n",
    "                    acc, f1, con_acc, con_f1 = evaluate_distillation(\n",
    "                        student,\n",
    "                        X_train_sampled,\n",
    "                        X_test,\n",
    "                        train_labels_sampled,\n",
    "                        y_test,\n",
    "                        y_test_distilled,\n",
    "                        description=desc\n",
    "                    )\n",
    "\n",
    "                    # Save results\n",
    "                    meta = dict(\n",
    "                        description=desc,\n",
    "                        model_type=model_type,\n",
    "                        model_names=model_names_subset,\n",
    "                        k=k,\n",
    "                        acc=acc,\n",
    "                        f1=f1,\n",
    "                        con_acc=con_acc,\n",
    "                        con_f1=con_f1,\n",
    "                        num_nodes=\"\",\n",
    "                        max_depth=\"\",\n",
    "                        ccp=\"\"\n",
    "                    )\n",
    "\n",
    "                    if model_type == \"dt\":\n",
    "                        meta.update(\n",
    "                            num_nodes=student.tree_.node_count,\n",
    "                            max_depth=student.tree_.max_depth,\n",
    "                            ccp=ccp_alpha,\n",
    "                        )\n",
    "\n",
    "                    save_evaluation_results(**meta)\n",
    "\n",
    "                    # Save model + features\n",
    "                    with open(model_path, \"wb\") as f:\n",
    "                        pickle.dump(student, f)\n",
    "                    with open(model_features_path, \"wb\") as f:\n",
    "                        pickle.dump(features, f)\n",
    "\n",
    "                    # Export string representation if DT\n",
    "                    if model_type == \"dt\":\n",
    "                        tree_str = tree_to_str(student, feature_names=features, class_names=class_names)\n",
    "                        with open(model_str_path, \"w\") as f:\n",
    "                            f.write(tree_str)\n",
    "\n",
    "                    # Save predictions\n",
    "                    y_pred = student.predict(X_test)\n",
    "                    np.save(model_y_path, y_pred)\n",
    "\n",
    "print(\"Analysis complete.\")\n",
    "acc_tp = accuracy_score(y_test_distilled, y_test)\n",
    "f1_score_tp = f1_score(y_test_distilled, y_test, average=\"weighted\")\n",
    "print(f\"Tappbert baseline: acc - {acc_tp:.4f}, f1 - {f1_score_tp:.4f}\")\n",
    "print(\"Done and dusted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de85773f",
   "metadata": {},
   "source": [
    "# Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6a576a",
   "metadata": {},
   "source": [
    "### Load Tree and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c04a9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating the git repository...\n",
      "Already up to date.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Structure:\n",
      "0: event -1 activity question > 0.5000 T: 1, F: 72\n",
      "1: event -1 activity werkmap > 0.5000 T: 2, F: 63\n",
      "2: event -1 activity mijn sollicitaties > 0.5000 T: 3, F: 62\n",
      "3: event -1 activity home > 0.5000 T: 4, F: 45\n",
      "4: event -2 activity question > 0.5000 T: 5, F: 40\n",
      "5: event -3 age=18-29 > 0.0208 T: 6, F: 7\n",
      "6: predict question\n",
      "7: event -1 activity taken > 0.5000 T: 8, F: 23\n",
      "8: event -1 activity mijn documenten > 0.5000 T: 9, F: 20\n",
      "9: event -1 activity mijn berichten > 0.5000 T: 10, F: 13\n",
      "10: event -1 activity aanvragen-ww > 0.5000 T: 11, F: 12\n",
      "11: predict mijn_sollicitaties\n",
      "12: predict home\n",
      "13: event -1 age=50-65 > 0.5000 T: 14, F: 17\n",
      "14: event -4 time since midnight scaled > 0.0000 T: 15, F: 16\n",
      "15: predict mijn_sollicitaties\n",
      "16: predict mijn_sollicitaties\n",
      "17: event -2 activity home > 0.5000 T: 18, F: 19\n",
      "18: predict mijn_sollicitaties\n",
      "19: predict home\n",
      "20: event -4 age=18-29 > 0.0238 T: 21, F: 22\n",
      "21: predict mijn_werkmap\n",
      "22: predict mijn_sollicitaties\n",
      "23: event -3 age=18-29 > 0.9495 T: 24, F: 39\n",
      "24: event -2 activity werkmap > 0.5000 T: 25, F: 38\n",
      "25: event -2 activity home > 0.5000 T: 26, F: 33\n",
      "26: event -3 activity question > 0.5000 T: 27, F: 32\n",
      "27: event -3 time since midnight scaled > 0.0005 T: 28, F: 29\n",
      "28: predict home\n",
      "29: event -2 activity mijn berichten > 0.5000 T: 30, F: 31\n",
      "30: predict mijn_sollicitaties\n",
      "31: predict home\n",
      "32: predict mijn_sollicitaties\n",
      "33: event -2 age=50-65 > 0.5000 T: 34, F: 37\n",
      "34: event -3 time since midnight scaled > 0.0000 T: 35, F: 36\n",
      "35: predict mijn_sollicitaties\n",
      "36: predict home\n",
      "37: predict home\n",
      "38: predict home\n",
      "39: predict mijn_sollicitaties\n",
      "40: event -1 activity aanvragen-ww > 0.5000 T: 41, F: 44\n",
      "41: event -1 activity inschrijven > 0.5000 T: 42, F: 43\n",
      "42: predict mijn_sollicitaties\n",
      "43: predict aanvragen-ww\n",
      "44: predict home\n",
      "45: event -2 activity question > 0.5000 T: 46, F: 49\n",
      "46: event -3 age=18-29 > 0.0000 T: 47, F: 48\n",
      "47: predict taken\n",
      "48: predict mijn_sollicitaties\n",
      "49: event -1 time since midnight scaled > 0.0029 T: 50, F: 57\n",
      "50: event -3 age=18-29 > 0.0238 T: 51, F: 54\n",
      "51: event -2 BoNG want cancel > 0.1237 T: 52, F: 53\n",
      "52: predict taken\n",
      "53: predict mijn_werkmap\n",
      "54: event -3 activity question > 0.5000 T: 55, F: 56\n",
      "55: predict mijn_sollicitaties\n",
      "56: predict taken\n",
      "57: event -2 BoNG want cancel > 0.1237 T: 58, F: 61\n",
      "58: event -2 BoNG consultant work > 0.1549 T: 59, F: 60\n",
      "59: predict taken\n",
      "60: predict mijn_werkmap\n",
      "61: predict mijn_werkmap\n",
      "62: predict taken\n",
      "63: event -2 BoNG want cancel > 0.1237 T: 64, F: 71\n",
      "64: event -2 age=18-29 > 0.0476 T: 65, F: 66\n",
      "65: predict mijn_werkmap\n",
      "66: event -2 BoNG consultant work > 0.1549 T: 67, F: 70\n",
      "67: event -2 BoNG report change > 0.3578 T: 68, F: 69\n",
      "68: predict taken\n",
      "69: predict mijn_werkmap\n",
      "70: predict mijn_werkmap\n",
      "71: predict mijn_werkmap\n",
      "72: event -1 BoNG want cancel > 0.1237 T: 73, F: 92\n",
      "73: event -1 BoNG consultant work > 0.1549 T: 74, F: 91\n",
      "74: event -2 activity taken > 0.5000 T: 75, F: 90\n",
      "75: event -2 activity mijn berichten > 0.5000 T: 76, F: 89\n",
      "76: event -2 BoNG want cancel > 0.1237 T: 77, F: 88\n",
      "77: event -3 activity taken > 0.5000 T: 78, F: 87\n",
      "78: event -4 time since midnight scaled > 0.0000 T: 79, F: 86\n",
      "79: event -2 BoNG consultant work > 0.1549 T: 80, F: 85\n",
      "80: event -2 activity aanvragen-ww > 0.5000 T: 81, F: 84\n",
      "81: event -3 BoNG consultant work > 0.1331 T: 82, F: 83\n",
      "82: predict question\n",
      "83: predict mijn_werkmap\n",
      "84: predict aanvragen-ww\n",
      "85: predict mijn_werkmap\n",
      "86: predict taken\n",
      "87: predict home\n",
      "88: predict mijn_werkmap\n",
      "89: predict mijn_werkmap\n",
      "90: predict taken\n",
      "91: predict mijn_werkmap\n",
      "92: predict mijn_werkmap\n"
     ]
    }
   ],
   "source": [
    "update_repository()\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tapp.text_encoder import BoWTextEncoder\n",
    "from tapp.text_encoder import BoNGTextEncoder\n",
    "from tapp.text_encoder import LDATextEncoder\n",
    "from tapp.text_encoder import BERTbaseTextEncoder\n",
    "from tapp.log_encoder import LogEncoder\n",
    "from distillation import (\n",
    "    get_distillation_paths,\n",
    "    get_evaluation_paths,\n",
    "    evaluate_distillation,\n",
    "    analyze_text_splits,\n",
    "    prepare_text_feature_datasets,\n",
    "    concatenate_text_feature_datasets,\n",
    "    get_feature_datasets,\n",
    "    save_evaluation_results,\n",
    "    tree_to_str,\n",
    "    explain_sample\n",
    ")\n",
    "import numpy as np\n",
    "import sys\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "text_models = [\n",
    "    BoWTextEncoder(encoding_length=50, language=\"english\"),\n",
    "    BoNGTextEncoder(n=2, encoding_length=50, language=\"english\"),\n",
    "    LDATextEncoder(encoding_length=100, language=\"english\"),\n",
    "]\n",
    "version = \"distilled\"\n",
    "model_names = [\"BoW\", \"BoNG\"]\n",
    "model_names = [\"BoNG\"]\n",
    "model_type = \"dt\"\n",
    "k = 5\n",
    "ccp_alpha = 0.001\n",
    "\n",
    "tree_model_path, tree_str_path, tree_features_path, tree_y_path = get_evaluation_paths(\n",
    "    version,\n",
    "    model_type=model_type,\n",
    "    model_names_subset=model_names,\n",
    "    k=k,\n",
    "    ccp_alpha=ccp_alpha,\n",
    ")\n",
    "#text_model_tapp = BERTbaseFineTunedNextActivityTextEncoder(encoding_length=768, language=\"english\", epochs=16, lr=5e-5)\n",
    "text_model_tapp = BERTbaseTextEncoder(encoding_length=768, language=\"english\")\n",
    "\n",
    "# get the x_test samples\n",
    "train_dataset, test_dataset = get_feature_datasets(\n",
    "    text_models,\n",
    "    log,\n",
    "    train_log,\n",
    "    test_log,\n",
    "    k=k,\n",
    ")\n",
    "X_test, features = concatenate_text_feature_datasets(test_dataset, model_names)\n",
    "\n",
    "# load the distilled labels\n",
    "_, distillation_path_test = get_distillation_paths(\n",
    "    text_model_tapp\n",
    ")\n",
    "y_test_distilled_raw = np.load(distillation_path_test)\n",
    "y_test_distilled = y_test_distilled_raw.argmax(axis=1)\n",
    "\n",
    "# load the tree model\n",
    "with open(tree_model_path, \"rb\") as f:\n",
    "    dt = pickle.load(f)\n",
    "# load the tree predictions\n",
    "y_tree = np.load(tree_y_path)\n",
    "\n",
    "tree_str = tree_to_str(dt, feature_names=features, class_names=class_names)\n",
    "print(\"Decision Tree Structure:\")\n",
    "print(tree_str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3144ee09",
   "metadata": {},
   "source": [
    "### Analyze Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60cc685f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating the git repository...\n",
      "Already up to date.\n",
      "\n",
      "Sample:\n",
      "event -2 activity question = 1, event -2 time since monday scaled = 0.5006, event -2 age=18-29 = 0.3572, event -2 age=50-65 = 1, event -1 activity question = 1, event -1 time since midnight scaled = 0.0007, event -1 time since monday scaled = 0.5006, event -1 age=18-29 = 0.3572, event -1 age=50-65 = 1\n",
      "Correct activity: question\n",
      "Top 3 predicted activities: question: 0.303, home: 0.164, taken: 0.150, werkmap: 0.114, mijn_berichten: 0.111\n",
      "Decision path:\n",
      "Node 0: (event_-1_activity_question = 1.000 > 0.500) | class distribution = {question: 15748, aanvragen-ww: 101, home: 3432, taken: 5587, vacatures_bij_mijn_cv: 2, mijn_berichten: 2902, wijziging_doorgeven: 12, mijn_cv: 21, vacatures_zoeken: 2, END: 16433}\n",
      "Node 72: (event_-1_BoNG_want cancel = 0.000 <= 0.124) | class distribution = {question: 14882, aanvragen-ww: 26, home: 321, taken: 281, vacatures_bij_mijn_cv: 0, mijn_berichten: 1933, wijziging_doorgeven: 0, mijn_cv: 21, vacatures_zoeken: 2, END: 44}\n",
      "Node 73: (event_-1_BoNG_consultant work = 0.000 <= 0.155) | class distribution = {question: 14882, aanvragen-ww: 26, home: 317, taken: 281, vacatures_bij_mijn_cv: 0, mijn_berichten: 1001, wijziging_doorgeven: 0, mijn_cv: 21, vacatures_zoeken: 2, END: 44}\n",
      "Node 74: (event_-2_activity_taken = 0.000 <= 0.500) | class distribution = {question: 14861, aanvragen-ww: 26, home: 314, taken: 280, vacatures_bij_mijn_cv: 0, mijn_berichten: 605, wijziging_doorgeven: 0, mijn_cv: 21, vacatures_zoeken: 2, END: 44}\n",
      "Node 75: (event_-2_activity_mijn_berichten = 0.000 <= 0.500) | class distribution = {question: 14854, aanvragen-ww: 26, home: 264, taken: 52, vacatures_bij_mijn_cv: 0, mijn_berichten: 603, wijziging_doorgeven: 0, mijn_cv: 21, vacatures_zoeken: 1, END: 44}\n",
      "Node 76: (event_-2_BoNG_want cancel = 0.000 <= 0.124) | class distribution = {question: 14854, aanvragen-ww: 26, home: 146, taken: 52, vacatures_bij_mijn_cv: 0, mijn_berichten: 459, wijziging_doorgeven: 0, mijn_cv: 21, vacatures_zoeken: 2, END: 44}\n",
      "Node 77: (event_-3_activity_taken = 0.000 <= 0.500) | class distribution = {question: 14854, aanvragen-ww: 26, home: 146, taken: 52, vacatures_bij_mijn_cv: 0, mijn_berichten: 322, wijziging_doorgeven: 0, mijn_cv: 21, vacatures_zoeken: 2, END: 44}\n",
      "Node 78: (event_-4_time_since_midnight_scaled = 0.000 <= 0.000) | class distribution = {question: 14854, aanvragen-ww: 26, home: 82, taken: 51, vacatures_bij_mijn_cv: 0, mijn_berichten: 321, wijziging_doorgeven: 0, mijn_cv: 18, vacatures_zoeken: 2, END: 42}\n",
      "Node 79: (event_-2_BoNG_consultant work = 0.000 <= 0.155) | class distribution = {question: 14854, aanvragen-ww: 24, home: 82, taken: 0, vacatures_bij_mijn_cv: 0, mijn_berichten: 298, wijziging_doorgeven: 0, mijn_cv: 18, vacatures_zoeken: 2, END: 37}\n",
      "Node 80: (event_-2_activity_aanvragen-ww = 0.000 <= 0.500) | class distribution = {question: 14848, aanvragen-ww: 24, home: 82, taken: 0, vacatures_bij_mijn_cv: 0, mijn_berichten: 246, wijziging_doorgeven: 0, mijn_cv: 18, vacatures_zoeken: 2, END: 37}\n",
      "Node 81: (event_-3_BoNG_consultant work = 0.000 <= 0.133) | class distribution = {question: 14847, aanvragen-ww: 0, home: 81, taken: 0, vacatures_bij_mijn_cv: 0, mijn_berichten: 245, wijziging_doorgeven: 0, mijn_cv: 17, vacatures_zoeken: 1, END: 36}\n",
      "--> Reached leaf node 82 with prediction: question | class distribution = {question: 14847, aanvragen-ww: 0, home: 82, taken: 0, vacatures_bij_mijn_cv: 0, mijn_berichten: 221, wijziging_doorgeven: 0, mijn_cv: 18, vacatures_zoeken: 1, END: 37}\n",
      "explain first few samples:\n",
      "event -1 activity question = 1, event -1 time since monday scaled = 0.3752, event -1 age=18-29 = 0.6250, event -1 age=50-65 = 1\n",
      "question\n",
      "event -2 activity question = 1, event -2 time since monday scaled = 0.3752, event -2 age=18-29 = 0.6250, event -2 age=50-65 = 1, event -1 activity wijziging doorgeven = 1, event -1 time since midnight scaled = 0.0145, event -1 time since monday scaled = 0.4172, event -1 age=18-29 = 0.6310, event -1 age=50-65 = 1\n",
      "END\n",
      "event -1 activity question = 1, event -1 time since monday scaled = 0.3756, event -1 age=18-29 = 0.6251, event -1 age=50-65 = 1\n",
      "question\n",
      "event -2 activity question = 1, event -2 time since monday scaled = 0.3756, event -2 age=18-29 = 0.6251, event -2 age=50-65 = 1, event -1 activity mijn berichten = 1, event -1 time since midnight scaled = 0.1111, event -1 time since monday scaled = 0.7089, event -1 age=18-29 = 0.6727, event -1 age=50-65 = 1\n",
      "END\n",
      "event -3 activity question = 1, event -3 time since monday scaled = 0.3756, event -3 age=18-29 = 0.6251, event -3 age=50-65 = 1, event -2 activity mijn berichten = 1, event -2 time since midnight scaled = 0.1111, event -2 time since monday scaled = 0.7089, event -2 age=18-29 = 0.6727, event -2 age=50-65 = 1, event -1 activity taken = 1, event -1 time since midnight scaled = 0.0001, event -1 time since monday scaled = 0.7085, event -1 age=18-29 = 0.6726, event -1 age=50-65 = 1\n",
      "END\n"
     ]
    }
   ],
   "source": [
    "update_repository()\n",
    "from distillation import explain_sample, get_decision_path\n",
    "\n",
    "k_highest_classes = 5\n",
    "\n",
    "class_names = np.array(class_names)\n",
    "conforming_indices = np.where(y_tree == y_test_distilled)[0]\n",
    "random_index = np.random.choice(conforming_indices, 1)[0]\n",
    "#print(f\"Randomly selected index: {random_index}\")\n",
    "random_X = X_test[random_index]\n",
    "random_y = y_test_distilled[random_index]\n",
    "random_y_raw = y_test_distilled_raw[random_index]\n",
    "# get the k highest probabilities\n",
    "topk_idx = np.argsort(random_y_raw)[-k_highest_classes:][::-1]\n",
    "topk_values = random_y_raw[topk_idx]\n",
    "topk_classes = class_names[topk_idx]\n",
    "topk_classes_str = \", \".join(f\"{cls}: {val:.3f}\" for cls, val in zip(topk_classes, topk_values))\n",
    "\n",
    "correct_activity = class_names[random_y]\n",
    "#print(\"Randomly selected sample:\")\n",
    "#print(explain_sample(random_X, features))\n",
    "#print(\"Correct activity:\", correct_activity)\n",
    "decision_path = get_decision_path(dt, random_X, feature_names=features, class_names=class_names, show_distribution=True)\n",
    "print(\"Sample:\")\n",
    "print(explain_sample(random_X, features))\n",
    "print(\"Correct activity:\", correct_activity)\n",
    "print(\"Top 3 predicted activities:\", topk_classes_str)\n",
    "print(\"Decision path:\")\n",
    "print(decision_path)\n",
    "\n",
    "print(\"explain first few samples:\")\n",
    "print(explain_sample(X_test[0], features))\n",
    "print(class_names[y_test_distilled[0]])\n",
    "print(explain_sample(X_test[1], features))\n",
    "print(class_names[y_test_distilled[1]])\n",
    "print(explain_sample(X_test[2], features))\n",
    "print(class_names[y_test_distilled[2]])\n",
    "print(explain_sample(X_test[3], features))\n",
    "print(class_names[y_test_distilled[3]])\n",
    "print(explain_sample(X_test[4], features))\n",
    "print(class_names[y_test_distilled[4]])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6625eb9",
   "metadata": {},
   "source": [
    "### Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a065392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant that provides explanations for decision tree predictions. Your task is to explain the decision-making process of a machine learning model, that has been distilled into a decision tree. \n",
      "Your explanation should be clear, concise, and focused on how the features contribute to the prediction. Make sure not to ground your explanation on the decision tree architecture itself, since the tree is only a proxy for the actual model, and provide higher level reasoning for the prediction instead. Be very precise in your explanation and leave no ambiguities. \n",
      "You will receive a sample represented by its features (assume unprovided features are 0) and the corresponding predicted class label, as well as the path the sample took in the decision tree itself. For every node in the path, you will see the class distribution of samples belonging to that node. Use this path to explain the decision process step by step how certain classes have been ruled out. \n",
      "Additionally, you will receive the 5 classes with the highest prediction probability. Provide an explanation of why the chosen class was predicted over the other top classes.\n",
      "Sample features: \n",
      " event -2 activity question = 1, event -2 time since monday scaled = 0.5006, event -2 age=18-29 = 0.3572, event -2 age=50-65 = 1, event -1 activity question = 1, event -1 time since midnight scaled = 0.0007, event -1 time since monday scaled = 0.5006, event -1 age=18-29 = 0.3572, event -1 age=50-65 = 1 \n",
      "Predicted class label: question \n",
      "Top 5 predicted classes: Top 5 predicted classes: question: 0.303, home: 0.164, taken: 0.150, werkmap: 0.114, mijn_berichten: 0.111 \n",
      " \n",
      "Decision path taken: \n",
      " Node 0: (event_-1_activity_question = 1.000 > 0.500) | class distribution = {question: 15748, aanvragen-ww: 101, home: 3432, taken: 5587, vacatures_bij_mijn_cv: 2, mijn_berichten: 2902, wijziging_doorgeven: 12, mijn_cv: 21, vacatures_zoeken: 2, END: 16433}\n",
      "Node 72: (event_-1_BoNG_want cancel = 0.000 <= 0.124) | class distribution = {question: 14882, aanvragen-ww: 26, home: 321, taken: 281, vacatures_bij_mijn_cv: 0, mijn_berichten: 1933, wijziging_doorgeven: 0, mijn_cv: 21, vacatures_zoeken: 2, END: 44}\n",
      "Node 73: (event_-1_BoNG_consultant work = 0.000 <= 0.155) | class distribution = {question: 14882, aanvragen-ww: 26, home: 317, taken: 281, vacatures_bij_mijn_cv: 0, mijn_berichten: 1001, wijziging_doorgeven: 0, mijn_cv: 21, vacatures_zoeken: 2, END: 44}\n",
      "Node 74: (event_-2_activity_taken = 0.000 <= 0.500) | class distribution = {question: 14861, aanvragen-ww: 26, home: 314, taken: 280, vacatures_bij_mijn_cv: 0, mijn_berichten: 605, wijziging_doorgeven: 0, mijn_cv: 21, vacatures_zoeken: 2, END: 44}\n",
      "Node 75: (event_-2_activity_mijn_berichten = 0.000 <= 0.500) | class distribution = {question: 14854, aanvragen-ww: 26, home: 264, taken: 52, vacatures_bij_mijn_cv: 0, mijn_berichten: 603, wijziging_doorgeven: 0, mijn_cv: 21, vacatures_zoeken: 1, END: 44}\n",
      "Node 76: (event_-2_BoNG_want cancel = 0.000 <= 0.124) | class distribution = {question: 14854, aanvragen-ww: 26, home: 146, taken: 52, vacatures_bij_mijn_cv: 0, mijn_berichten: 459, wijziging_doorgeven: 0, mijn_cv: 21, vacatures_zoeken: 2, END: 44}\n",
      "Node 77: (event_-3_activity_taken = 0.000 <= 0.500) | class distribution = {question: 14854, aanvragen-ww: 26, home: 146, taken: 52, vacatures_bij_mijn_cv: 0, mijn_berichten: 322, wijziging_doorgeven: 0, mijn_cv: 21, vacatures_zoeken: 2, END: 44}\n",
      "Node 78: (event_-4_time_since_midnight_scaled = 0.000 <= 0.000) | class distribution = {question: 14854, aanvragen-ww: 26, home: 82, taken: 51, vacatures_bij_mijn_cv: 0, mijn_berichten: 321, wijziging_doorgeven: 0, mijn_cv: 18, vacatures_zoeken: 2, END: 42}\n",
      "Node 79: (event_-2_BoNG_consultant work = 0.000 <= 0.155) | class distribution = {question: 14854, aanvragen-ww: 24, home: 82, taken: 0, vacatures_bij_mijn_cv: 0, mijn_berichten: 298, wijziging_doorgeven: 0, mijn_cv: 18, vacatures_zoeken: 2, END: 37}\n",
      "Node 80: (event_-2_activity_aanvragen-ww = 0.000 <= 0.500) | class distribution = {question: 14848, aanvragen-ww: 24, home: 82, taken: 0, vacatures_bij_mijn_cv: 0, mijn_berichten: 246, wijziging_doorgeven: 0, mijn_cv: 18, vacatures_zoeken: 2, END: 37}\n",
      "Node 81: (event_-3_BoNG_consultant work = 0.000 <= 0.133) | class distribution = {question: 14847, aanvragen-ww: 0, home: 81, taken: 0, vacatures_bij_mijn_cv: 0, mijn_berichten: 245, wijziging_doorgeven: 0, mijn_cv: 17, vacatures_zoeken: 1, END: 36}\n",
      "--> Reached leaf node 82 with prediction: question | class distribution = {question: 14847, aanvragen-ww: 0, home: 82, taken: 0, vacatures_bij_mijn_cv: 0, mijn_berichten: 221, wijziging_doorgeven: 0, mijn_cv: 18, vacatures_zoeken: 1, END: 37} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_sentences = 8\n",
    "\n",
    "role_description = \"You are a helpful assistant that provides explanations for machine learning model predictions. \" \\\n",
    "\"Your task is to explain the decision-making process of a machine learning model, \" \\\n",
    "\"that has been distilled into a decision tree. \\n\"\n",
    "\n",
    "task_description = f\"Your explanation should be clear, concise, \" \\\n",
    "\"and focused on how the features contribute to the prediction. \" \\\n",
    "\"Make sure not to ground your explanation on the decision tree architecture itself, \" \\\n",
    "\"since the tree is only a proxy for the actual model, \" \\\n",
    "\"and provide higher level reasoning for the prediction instead. \" \\\n",
    "\"Be very precise in your explanation and leave no ambiguities. \\n\"\n",
    "\n",
    "sentence_limit = \"Limit the description to {max_sentences} sentences.\\n\"\n",
    "\n",
    "input_description = \"You will receive a sample represented by its features (assume unprovided features are 0) \" \\\n",
    "\"and the corresponding predicted class label, as well as the path the sample took in the decision tree itself. \" \\\n",
    "\"For every node in the path, you will see the class distribution of samples belonging to that node. \" \\\n",
    "\"Use this path to explain the decision process step by step how certain classes have been ruled out. \\n\"\n",
    "\n",
    "topk_description = f\"Additionally, you will receive the {k} classes with the highest prediction probability. \" \\\n",
    "\"Provide an explanation of why the chosen class was predicted over the other top classes.\\n\"\n",
    "\n",
    "self_correction = \"Additionally, for the purpose of supervision, also provide the nodes that were traversed \" \\\n",
    "\"in the decision tree to reach the prediction. \\n\\n\"\n",
    "\n",
    "\n",
    "sample_description = f\"Sample features: \\n {explain_sample(random_X, features)} \\n\"\n",
    "prediction_description = f\"Predicted class label: {correct_activity} \\n\"\n",
    "tree_description = f\"Decision tree structure: {tree_str} \\n\"\n",
    "decision_path_str = f\"Decision path taken: \\n {decision_path} \\n\"\n",
    "topk_classes_str = f\"Top {k_highest_classes} predicted classes: {topk_classes_str} \\n\"\n",
    "\n",
    "#TODO: perhaps instruct to provide reasoning step by step\n",
    "#prompt = role_description + self_correction + sample_description + prediction_description + tree_description\n",
    "#prompt = role_description + sample_description + prediction_description + tree_description\n",
    "#prompt = f\"{role_description}{task_description}{input_description}{sample_description}{prediction_description}{tree_description}\"\n",
    "prompt = f\"{role_description}{task_description}{input_description}{topk_description}{sample_description}{prediction_description}{topk_classes_str}{decision_path_str}\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cfec38",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2766b178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activities: ['taken' 'question' 'mijn_werkmap' 'home' 'mijn_berichten'\n",
      " 'wijziging_doorgeven' 'werkmap' 'mijn_documenten' 'vacatures_bij_mijn_cv'\n",
      " 'mijn_sollicitaties' 'wdo' 'mijn_cv' 'aanvragen-ww' 'vacatures_zoeken'\n",
      " 'inschrijven' 'mijn_tips' 'vacatures' 'vragenlijst-uwv']\n",
      "Number of activities: 18\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18]\n",
      "19\n",
      "Class counts in training data: {0: 6252, 1: 242, 2: 6992, 3: 6408, 4: 1088, 5: 767, 6: 4790, 7: 2103, 8: 690, 9: 1104, 10: 600, 11: 558, 12: 82, 13: 319, 14: 34, 15: 80, 16: 31, 17: 100, 18: 12000}\n",
      "Class counts in test data: {0: 1587, 1: 32, 2: 1759, 3: 1577, 4: 269, 5: 146, 6: 1260, 7: 542, 8: 175, 9: 282, 10: 146, 11: 91, 12: 12, 13: 59, 14: 9, 15: 16, 16: 4, 17: 13, 18: 3001}\n"
     ]
    }
   ],
   "source": [
    "print(f\"activities: {activities}\")\n",
    "print(f\"Number of activities: {len(activities)}\")\n",
    "print(dt.classes_)\n",
    "print(len(dt.classes_))\n",
    "\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "class_counts = dict(zip(unique, counts))\n",
    "print(\"Class counts in training data:\", class_counts)\n",
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "class_counts = dict(zip(unique, counts))\n",
    "print(\"Class counts in test data:\", class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24615e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conformance between base and fine-tuned distilled labels: 0.8415\n",
      "Base distilled labels: acc - 0.4857, f1 - 0.5491\n",
      "Fine-tuned distilled labels: acc - 0.4848, f1 - 0.5341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# load the distilled labels\n",
    "text_model_tapp = BERTbaseTextEncoder(encoding_length=768, language=\"english\")\n",
    "distillation_path_train, distillation_path_test = get_distillation_paths(\n",
    "    text_model_tapp\n",
    ")\n",
    "y_train_distilled = np.load(distillation_path_train)\n",
    "y_test_distilled = np.load(distillation_path_test)\n",
    "y_train_distilled_base = y_train_distilled.argmax(axis=1)\n",
    "y_test_distilled_base = y_test_distilled.argmax(axis=1)\n",
    "\n",
    "text_model_tapp = BERTbaseFineTunedNextActivityTextEncoder(encoding_length=768, language=\"english\", epochs=16, lr=5e-5)\n",
    "distillation_path_train, distillation_path_test = get_distillation_paths(\n",
    "    text_model_tapp\n",
    ")\n",
    "y_train_distilled = np.load(distillation_path_train)\n",
    "y_test_distilled = np.load(distillation_path_test)\n",
    "y_train_distilled_tuned = y_train_distilled.argmax(axis=1)\n",
    "y_test_distilled_tuned = y_test_distilled.argmax(axis=1)\n",
    "\n",
    "conformance = accuracy_score(y_test_distilled_base, y_test_distilled_tuned)\n",
    "print(f\"Conformance between base and fine-tuned distilled labels: {conformance:.4f}\")\n",
    "acc_base = accuracy_score(y_test_distilled_base, y_test)\n",
    "f1_base = f1_score(y_test_distilled_base, y_test, average=\"weighted\")\n",
    "acc_tuned = accuracy_score(y_test_distilled_tuned, y_test)\n",
    "f1_tuned = f1_score(y_test_distilled_tuned, y_test, average=\"weighted\")\n",
    "print(f\"Base distilled labels: acc - {acc_base:.4f}, f1 - {f1_base:.4f}\")\n",
    "print(f\"Fine-tuned distilled labels: acc - {acc_tuned:.4f}, f1 - {f1_tuned:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b0c886",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m X_train_indie, y_train_indie, _ \u001b[38;5;241m=\u001b[39m log_encoder\u001b[38;5;241m.\u001b[39mtransform(train_log, for_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     15\u001b[0m X_test_indie, y_test_indie, _ \u001b[38;5;241m=\u001b[39m log_encoder\u001b[38;5;241m.\u001b[39mtransform(test_log, for_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 16\u001b[0m X_train_indie \u001b[38;5;241m=\u001b[39m \u001b[43mlog_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_indie\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m X_test_indie \u001b[38;5;241m=\u001b[39m log_encoder\u001b[38;5;241m.\u001b[39mtransform_tree(X_test_indie, k\u001b[38;5;241m=\u001b[39mk)\n\u001b[1;32m     19\u001b[0m train_dataset, test_dataset \u001b[38;5;241m=\u001b[39m get_feature_datasets(\n\u001b[1;32m     20\u001b[0m     text_models,\n\u001b[1;32m     21\u001b[0m     log,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     k\u001b[38;5;241m=\u001b[39mk,\n\u001b[1;32m     25\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "k=3\n",
    "text_encoder = BoWTextEncoder(encoding_length=50, language=\"english\")\n",
    "log_encoder = LogEncoder(\n",
    "    text_encoder=text_encoder,\n",
    "    advanced_time_attributes=True,\n",
    "    text_base_for_training=\"event\",\n",
    ")\n",
    "log_encoder.fit(\n",
    "    log,\n",
    "    activities=activities,\n",
    "    data_attributes=[\"age\"],\n",
    "    text_attribute=\"question\",\n",
    ")\n",
    "X_train_indie, y_train_indie, _ = log_encoder.transform(train_log, for_training=True)\n",
    "X_test_indie, y_test_indie, _ = log_encoder.transform(test_log, for_training=True)\n",
    "X_train_indie = log_encoder.transform_tree(X_train_indie, k=k)\n",
    "X_test_indie = log_encoder.transform_tree(X_test_indie, k=k)\n",
    "\n",
    "train_dataset, test_dataset = get_feature_datasets(\n",
    "    text_models,\n",
    "    log,\n",
    "    train_log,\n",
    "    test_log,\n",
    "    k=k,\n",
    ")\n",
    "y_train_combo = train_dataset[\"y\"]\n",
    "y_test_combo = test_dataset[\"y\"]\n",
    "X_train_combo, features = concatenate_text_feature_datasets(train_dataset, [text_encoder.name])\n",
    "X_test_combo, _ = concatenate_text_feature_datasets(test_dataset, [text_encoder.name])\n",
    "X_train_combo = train_dataset[\"None\"][\"X\"]\n",
    "X_test_combo = test_dataset[\"None\"][\"X\"]\n",
    "\n",
    "\n",
    "def _mismatch_report(A, B, rtol=1e-5, atol=1e-8, max_show=25):\n",
    "    mask = ~(np.isclose(A, B, rtol=rtol, atol=atol) | (np.isnan(A) & np.isnan(B)))\n",
    "    idxs = np.argwhere(mask)\n",
    "    n_diffs = idxs.shape[0]\n",
    "    cols = np.unique(idxs[:, 1]) if idxs.size else np.array([], dtype=int)\n",
    "    rows = np.unique(idxs[:, 0]) if idxs.size else np.array([], dtype=int)\n",
    "    sample = []\n",
    "    for r, c in idxs[:max_show]:\n",
    "        sample.append((int(r), int(c), A[r, c], B[r, c]))\n",
    "    return n_diffs, rows, cols, sample\n",
    "\n",
    "print(\"Shapes:\")\n",
    "print(\"  X_train_indie:\", X_train_indie.shape, \"X_train_combo:\", X_train_combo.shape)\n",
    "print(\"  X_test_indie :\", X_test_indie.shape,  \"X_test_combo :\", X_test_combo.shape)\n",
    "print(\"  y_train_indie:\", y_train_indie.shape, \"y_train_combo:\", y_train_combo.shape)\n",
    "print(\"  y_test_indie :\", y_test_indie.shape,  \"y_test_combo :\", y_test_combo.shape)\n",
    "\n",
    "print(\"\\nTraining X differences:\")\n",
    "n_diffs_tr, rows_tr, cols_tr, sample_tr = _mismatch_report(X_train_indie, X_train_combo)\n",
    "print(f\"  total differing elements: {n_diffs_tr}\")\n",
    "print(f\"  unique differing rows: {len(rows_tr)} -> {rows_tr[:20]}\")\n",
    "print(f\"  unique differing cols: {len(cols_tr)} -> {cols_tr[:20]}\")\n",
    "if sample_tr:\n",
    "    print(\"  first mismatches (row, col, indie_val, combo_val):\")\n",
    "    for tup in sample_tr[:10]:\n",
    "        print(\"   \", tup)\n",
    "\n",
    "print(\"\\nTest X differences:\")\n",
    "n_diffs_te, rows_te, cols_te, sample_te = _mismatch_report(X_test_indie, X_test_combo)\n",
    "print(f\"  total differing elements: {n_diffs_te}\")\n",
    "print(f\"  unique differing rows: {len(rows_te)} -> {rows_te[:20]}\")\n",
    "print(f\"  unique differing cols: {len(cols_te)} -> {cols_te[:20]}\")\n",
    "if sample_te:\n",
    "    print(\"  first mismatches (row, col, indie_val, combo_val):\")\n",
    "    for tup in sample_te[:10]:\n",
    "        print(\"   \", tup)\n",
    "\n",
    "print(\"\\nTraining y differences:\")\n",
    "y_train_mask = ~(np.isclose(y_train_indie, y_train_combo) | (np.isnan(y_train_indie) & np.isnan(y_train_combo)))\n",
    "y_train_diffs = np.argwhere(y_train_mask)\n",
    "print(f\"  total differing elements: {y_train_diffs.shape[0]}\")\n",
    "if y_train_diffs.size:\n",
    "    rows = np.unique(y_train_diffs[:,0])\n",
    "    print(f\"  unique differing rows: {len(rows)} -> {rows[:20]}\")\n",
    "\n",
    "print(\"\\nTest y differences:\")\n",
    "y_test_mask = ~(np.isclose(y_test_indie, y_test_combo) | (np.isnan(y_test_indie) & np.isnan(y_test_combo)))\n",
    "y_test_diffs = np.argwhere(y_test_mask)\n",
    "print(f\"  total differing elements: {y_test_diffs.shape[0]}\")\n",
    "if y_test_diffs.size:\n",
    "    rows = np.unique(y_test_diffs[:,0])\n",
    "    print(f\"  unique differing rows: {len(rows)} -> {rows[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d5849ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "  X_train_indie: (44240, 25) X_train_combo: (44240, 25)\n",
      "  X_test_indie : (10980, 25) X_test_combo : (10980, 25)\n",
      "  y_train_indie: (44240, 19) y_train_combo: (44240, 19)\n",
      "  y_test_indie : (10980, 19) y_test_combo : (10980, 19)\n",
      "\n",
      "Training X differences:\n",
      "  total differing elements: 0\n",
      "  unique differing rows: 0 -> []\n",
      "  unique differing cols: 0 -> []\n",
      "\n",
      "Test X differences:\n",
      "  total differing elements: 8972\n",
      "  unique differing rows: 4486 -> [ 1  3  5  6  9 10 15 19 20 21 23 25 26 27 32 33 36 37 42 46]\n",
      "  unique differing cols: 15 -> [ 1  2  4  5  6  8  9 10 11 12 13 14 15 16 17]\n",
      "  first mismatches (row, col, indie_val, combo_val):\n",
      "    (1, 1, 0.0, 1.0)\n",
      "    (1, 8, 1.0, 0.0)\n",
      "    (3, 2, 0.0, 1.0)\n",
      "    (3, 6, 1.0, 0.0)\n",
      "    (5, 4, 0.0, 1.0)\n",
      "    (5, 9, 1.0, 0.0)\n",
      "    (6, 2, 1.0, 0.0)\n",
      "    (6, 5, 0.0, 1.0)\n",
      "    (9, 2, 0.0, 1.0)\n",
      "    (9, 6, 1.0, 0.0)\n",
      "\n",
      "Training y differences:\n",
      "  total differing elements: 0\n",
      "\n",
      "Test y differences:\n",
      "  total differing elements: 8546\n",
      "  unique differing rows: 4273 -> [ 0  2  4  5  8  9 14 18 19 20 22 24 25 26 31 32 35 36 41 45]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def _extract_y(ds):\n",
    "    return ds[\"y\"] if isinstance(ds, dict) else getattr(ds, \"y\")\n",
    "\n",
    "def _assert_same_shape(A, B, nameA=\"A\", nameB=\"B\"):\n",
    "    if A.shape != B.shape:\n",
    "        raise ValueError(f\"Shape mismatch: {nameA}.shape={A.shape} vs {nameB}.shape={B.shape}\")\n",
    "\n",
    "def _mismatch_report(A, B, rtol=1e-5, atol=1e-8, max_show=25):\n",
    "    mask = ~(np.isclose(A, B, rtol=rtol, atol=atol) | (np.isnan(A) & np.isnan(B)))\n",
    "    idxs = np.argwhere(mask)\n",
    "    n_diffs = idxs.shape[0]\n",
    "    cols = np.unique(idxs[:, 1]) if idxs.size else np.array([], dtype=int)\n",
    "    rows = np.unique(idxs[:, 0]) if idxs.size else np.array([], dtype=int)\n",
    "    sample = []\n",
    "    for r, c in idxs[:max_show]:\n",
    "        sample.append((int(r), int(c), A[r, c], B[r, c]))\n",
    "    return n_diffs, rows, cols, sample\n",
    "\n",
    "def compare_indie_vs_combo(\n",
    "    log_encoder,\n",
    "    train_log,\n",
    "    test_log,\n",
    "    train_dataset,\n",
    "    test_dataset,\n",
    "    concatenate_text_feature_datasets,\n",
    "    k,\n",
    "):\n",
    "    X_train_3d, y_train_indie, _ = log_encoder.transform(train_log, for_training=True)\n",
    "    X_test_3d,  y_test_indie,  _ = log_encoder.transform(test_log,  for_training=True)\n",
    "\n",
    "    # IMPORTANT: bring the \"indie\" path into the same flattened, k-limited space\n",
    "    X_train_indie = log_encoder.transform_tree(X_train_3d, k=k)\n",
    "    X_test_indie  = log_encoder.transform_tree(X_test_3d,  k=k)\n",
    "\n",
    "    X_train_combo, feat_train = concatenate_text_feature_datasets(train_dataset, [])\n",
    "    X_test_combo,  feat_test  = concatenate_text_feature_datasets(test_dataset,  [])\n",
    "\n",
    "    y_train_combo = _extract_y(train_dataset)\n",
    "    y_test_combo  = _extract_y(test_dataset)\n",
    "\n",
    "    print(\"Shapes:\")\n",
    "    print(\"  X_train_indie:\", X_train_indie.shape, \"X_train_combo:\", X_train_combo.shape)\n",
    "    print(\"  X_test_indie :\", X_test_indie.shape,  \"X_test_combo :\", X_test_combo.shape)\n",
    "    print(\"  y_train_indie:\", y_train_indie.shape, \"y_train_combo:\", y_train_combo.shape)\n",
    "    print(\"  y_test_indie :\", y_test_indie.shape,  \"y_test_combo :\", y_test_combo.shape)\n",
    "\n",
    "    _assert_same_shape(X_train_indie, X_train_combo, \"X_train_indie\", \"X_train_combo\")\n",
    "    _assert_same_shape(X_test_indie,  X_test_combo,  \"X_test_indie\",  \"X_test_combo\")\n",
    "    _assert_same_shape(y_train_indie, y_train_combo, \"y_train_indie\", \"y_train_combo\")\n",
    "    _assert_same_shape(y_test_indie,  y_test_combo,  \"y_test_indie\",  \"y_test_combo\")\n",
    "\n",
    "    print(\"\\nTraining X differences:\")\n",
    "    n_diffs_tr, rows_tr, cols_tr, sample_tr = _mismatch_report(X_train_indie, X_train_combo)\n",
    "    print(f\"  total differing elements: {n_diffs_tr}\")\n",
    "    print(f\"  unique differing rows: {len(rows_tr)} -> {rows_tr[:20]}\")\n",
    "    print(f\"  unique differing cols: {len(cols_tr)} -> {cols_tr[:20]}\")\n",
    "    if sample_tr:\n",
    "        print(\"  first mismatches (row, col, indie_val, combo_val):\")\n",
    "        for tup in sample_tr[:10]:\n",
    "            print(\"   \", tup)\n",
    "\n",
    "    print(\"\\nTest X differences:\")\n",
    "    n_diffs_te, rows_te, cols_te, sample_te = _mismatch_report(X_test_indie, X_test_combo)\n",
    "    print(f\"  total differing elements: {n_diffs_te}\")\n",
    "    print(f\"  unique differing rows: {len(rows_te)} -> {rows_te[:20]}\")\n",
    "    print(f\"  unique differing cols: {len(cols_te)} -> {cols_te[:20]}\")\n",
    "    if sample_te:\n",
    "        print(\"  first mismatches (row, col, indie_val, combo_val):\")\n",
    "        for tup in sample_te[:10]:\n",
    "            print(\"   \", tup)\n",
    "\n",
    "    print(\"\\nTraining y differences:\")\n",
    "    y_train_mask = ~(np.isclose(y_train_indie, y_train_combo) | (np.isnan(y_train_indie) & np.isnan(y_train_combo)))\n",
    "    y_train_diffs = np.argwhere(y_train_mask)\n",
    "    print(f\"  total differing elements: {y_train_diffs.shape[0]}\")\n",
    "    if y_train_diffs.size:\n",
    "        rows = np.unique(y_train_diffs[:,0])\n",
    "        print(f\"  unique differing rows: {len(rows)} -> {rows[:20]}\")\n",
    "\n",
    "    print(\"\\nTest y differences:\")\n",
    "    y_test_mask = ~(np.isclose(y_test_indie, y_test_combo) | (np.isnan(y_test_indie) & np.isnan(y_test_combo)))\n",
    "    y_test_diffs = np.argwhere(y_test_mask)\n",
    "    print(f\"  total differing elements: {y_test_diffs.shape[0]}\")\n",
    "    if y_test_diffs.size:\n",
    "        rows = np.unique(y_test_diffs[:,0])\n",
    "        print(f\"  unique differing rows: {len(rows)} -> {rows[:20]}\")\n",
    "\n",
    "    return {\n",
    "        \"train\": {\n",
    "            \"n_diffs\": n_diffs_tr,\n",
    "            \"rows\": rows_tr,\n",
    "            \"cols\": cols_tr,\n",
    "            \"sample\": sample_tr,\n",
    "            \"features\": feat_train,\n",
    "        },\n",
    "        \"test\": {\n",
    "            \"n_diffs\": n_diffs_te,\n",
    "            \"rows\": rows_te,\n",
    "            \"cols\": cols_te,\n",
    "            \"sample\": sample_te,\n",
    "            \"features\": feat_test,\n",
    "        },\n",
    "    }\n",
    "\n",
    "res = compare_indie_vs_combo(\n",
    "    log_encoder=log_encoder,\n",
    "    train_log=train_log,\n",
    "    test_log=test_log,\n",
    "    train_dataset=train_dataset,\n",
    "    test_dataset=test_dataset,\n",
    "    concatenate_text_feature_datasets=concatenate_text_feature_datasets,\n",
    "    k=k,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9c5361",
   "metadata": {},
   "source": [
    "# Log Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90f1a0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['gender', 'name', '@@case_index', 'disposition', 'ndc', '@@index', 'timestamps', 'rhythm', 'resprate', 'etc_rn', 'sbp', 'icd_title', 'med_rn', 'dbp', 'etcdescription', 'case:acuity', 'icd_code', 'gsn', 'time:timestamp', 'icd_version', 'etccode', 'case:concept:name', 'arrival_transport', 'pain', 'case:subject_id', 'case:chiefcomplaint', 'o2sat', 'stay_id', 'race', 'case:hadm_id', 'seq_num', 'gsn_rn', 'heartrate', 'temperature', 'activity', 'concept:name']\n",
      "Activities: ['Discharge from the ED', 'Enter the ED', 'Medicine dispensations', 'Medicine reconciliation', 'Triage in the ED', 'Vital sign check']\n",
      "Number of unique activities: 6\n",
      "Cases: 2000\n",
      "Total number of events: 27718\n",
      "Average number of events per case: 13.859\n",
      "Average event duration: 0 days 00:20:04.299166576\n",
      "Event log head:\n",
      "      case:concept:name             concept:name      time:timestamp\n",
      "11318          30005370             Enter the ED 2110-06-25 05:10:00\n",
      "11319          30005370         Triage in the ED 2110-06-25 05:10:01\n",
      "11320          30005370  Medicine reconciliation 2110-06-25 05:28:00\n",
      "11321          30005370  Medicine reconciliation 2110-06-25 05:28:00\n",
      "11322          30005370  Medicine reconciliation 2110-06-25 05:28:00\n",
      "11323          30005370  Medicine reconciliation 2110-06-25 05:28:00\n",
      "11324          30005370  Medicine reconciliation 2110-06-25 05:28:00\n",
      "11325          30005370  Medicine reconciliation 2110-06-25 05:28:00\n",
      "11326          30005370  Medicine reconciliation 2110-06-25 05:28:00\n",
      "11327          30005370  Medicine reconciliation 2110-06-25 05:28:00\n",
      "11328          30005370  Medicine reconciliation 2110-06-25 05:28:00\n",
      "11329          30005370  Medicine reconciliation 2110-06-25 05:29:00\n",
      "11330          30005370  Medicine reconciliation 2110-06-25 05:29:00\n",
      "11331          30005370   Medicine dispensations 2110-06-25 05:31:00\n",
      "11332          30005370   Medicine dispensations 2110-06-25 05:31:00\n",
      "11333          30005370   Medicine dispensations 2110-06-25 05:52:00\n",
      "11334          30005370   Medicine dispensations 2110-06-25 05:52:00\n",
      "11335          30005370   Medicine dispensations 2110-06-25 05:52:00\n",
      "11336          30005370         Vital sign check 2110-06-25 06:46:00\n",
      "11337          30005370         Vital sign check 2110-06-25 08:27:00\n",
      "11338          30005370   Medicine dispensations 2110-06-25 08:31:00\n",
      "11339          30005370    Discharge from the ED 2110-06-25 09:42:00\n",
      "16374          30011087             Enter the ED 2110-09-06 02:51:00\n",
      "16375          30011087         Triage in the ED 2110-09-06 02:51:01\n",
      "16376          30011087   Medicine dispensations 2110-09-06 03:04:00\n",
      "16377          30011087   Medicine dispensations 2110-09-06 03:04:00\n",
      "16378          30011087   Medicine dispensations 2110-09-06 03:10:00\n",
      "16379          30011087   Medicine dispensations 2110-09-06 03:10:00\n",
      "16380          30011087   Medicine dispensations 2110-09-06 03:10:00\n",
      "16381          30011087   Medicine dispensations 2110-09-06 03:10:00\n",
      "16382          30011087   Medicine dispensations 2110-09-06 03:10:00\n",
      "16383          30011087   Medicine dispensations 2110-09-06 03:10:00\n",
      "16384          30011087   Medicine dispensations 2110-09-06 03:17:00\n",
      "16385          30011087   Medicine dispensations 2110-09-06 03:27:00\n",
      "16386          30011087   Medicine dispensations 2110-09-06 03:27:00\n",
      "16387          30011087    Discharge from the ED 2110-09-06 05:09:56\n",
      "16388          30011087    Discharge from the ED 2110-09-06 05:09:56\n",
      "16389          30011087    Discharge from the ED 2110-09-06 05:09:56\n",
      "16390          30011087    Discharge from the ED 2110-09-06 05:09:56\n",
      "6661           30016618             Enter the ED 2110-04-26 00:33:00\n",
      "6662           30016618         Triage in the ED 2110-04-26 00:33:01\n",
      "6663           30016618         Vital sign check 2110-04-26 00:35:00\n",
      "6664           30016618         Vital sign check 2110-04-26 03:51:00\n",
      "6665           30016618   Medicine dispensations 2110-04-26 05:07:00\n",
      "6666           30016618         Vital sign check 2110-04-26 05:36:00\n",
      "6667           30016618   Medicine dispensations 2110-04-26 06:04:00\n",
      "6668           30016618    Discharge from the ED 2110-04-26 06:18:00\n",
      "6669           30016618    Discharge from the ED 2110-04-26 06:18:00\n",
      "6670           30016618    Discharge from the ED 2110-04-26 06:18:00\n",
      "6671           30016618    Discharge from the ED 2110-04-26 06:18:00\n",
      "15955          30022872             Enter the ED 2110-09-01 02:50:00\n",
      "15956          30022872         Triage in the ED 2110-09-01 02:50:01\n",
      "15957          30022872  Medicine reconciliation 2110-09-01 03:03:00\n",
      "15958          30022872  Medicine reconciliation 2110-09-01 03:03:00\n",
      "15959          30022872  Medicine reconciliation 2110-09-01 03:03:00\n",
      "15960          30022872  Medicine reconciliation 2110-09-01 03:03:00\n",
      "15961          30022872  Medicine reconciliation 2110-09-01 03:03:00\n",
      "15962          30022872  Medicine reconciliation 2110-09-01 03:03:00\n",
      "15963          30022872  Medicine reconciliation 2110-09-01 03:03:00\n",
      "15964          30022872   Medicine dispensations 2110-09-01 03:20:00\n",
      "15965          30022872         Vital sign check 2110-09-01 04:15:00\n",
      "15966          30022872         Vital sign check 2110-09-01 04:40:00\n",
      "15967          30022872   Medicine dispensations 2110-09-01 04:45:00\n",
      "15968          30022872         Vital sign check 2110-09-01 04:48:00\n",
      "15969          30022872   Medicine dispensations 2110-09-01 04:56:00\n",
      "15970          30022872         Vital sign check 2110-09-01 05:29:00\n",
      "15971          30022872         Vital sign check 2110-09-01 06:02:00\n",
      "15972          30022872         Vital sign check 2110-09-01 09:30:00\n",
      "15973          30022872    Discharge from the ED 2110-09-01 10:30:00\n",
      "15974          30022872    Discharge from the ED 2110-09-01 10:30:00\n",
      "15975          30022872    Discharge from the ED 2110-09-01 10:30:00\n",
      "22116          30023012             Enter the ED 2110-11-25 14:52:00\n",
      "22117          30023012         Triage in the ED 2110-11-25 14:52:01\n",
      "22118          30023012         Vital sign check 2110-11-25 14:55:00\n",
      "22119          30023012  Medicine reconciliation 2110-11-25 15:43:00\n",
      "22120          30023012  Medicine reconciliation 2110-11-25 15:43:00\n",
      "22121          30023012  Medicine reconciliation 2110-11-25 15:43:00\n",
      "22122          30023012  Medicine reconciliation 2110-11-25 15:43:00\n",
      "22123          30023012  Medicine reconciliation 2110-11-25 15:43:00\n",
      "22124          30023012  Medicine reconciliation 2110-11-25 15:43:00\n",
      "22125          30023012  Medicine reconciliation 2110-11-25 15:43:00\n",
      "22126          30023012  Medicine reconciliation 2110-11-25 15:43:00\n",
      "22127          30023012  Medicine reconciliation 2110-11-25 15:43:00\n",
      "22128          30023012  Medicine reconciliation 2110-11-25 15:43:00\n",
      "22129          30023012         Vital sign check 2110-11-25 18:47:00\n",
      "22130          30023012    Discharge from the ED 2110-11-25 20:38:00\n",
      "22131          30023012    Discharge from the ED 2110-11-25 20:38:00\n",
      "5346           30028898             Enter the ED 2110-04-08 20:22:00\n",
      "5347           30028898         Triage in the ED 2110-04-08 20:22:01\n",
      "5348           30028898         Vital sign check 2110-04-08 20:25:00\n",
      "5349           30028898   Medicine dispensations 2110-04-08 21:35:00\n",
      "5350           30028898   Medicine dispensations 2110-04-08 21:35:00\n",
      "5351           30028898   Medicine dispensations 2110-04-08 21:35:00\n",
      "5352           30028898   Medicine dispensations 2110-04-08 21:46:00\n",
      "5353           30028898   Medicine dispensations 2110-04-08 21:48:00\n",
      "5354           30028898   Medicine dispensations 2110-04-08 21:49:00\n",
      "5355           30028898   Medicine dispensations 2110-04-08 21:49:00\n",
      "5356           30028898  Medicine reconciliation 2110-04-08 22:08:00\n",
      "5357           30028898  Medicine reconciliation 2110-04-08 22:08:00\n",
      "5358           30028898  Medicine reconciliation 2110-04-08 22:08:00\n",
      "\n",
      "Case ID: 30005370\n",
      "      case:concept:name             concept:name      time:timestamp\n",
      "11318          30005370             Enter the ED 2110-06-25 05:10:00\n",
      "11319          30005370         Triage in the ED 2110-06-25 05:10:01\n",
      "11320          30005370  Medicine reconciliation 2110-06-25 05:28:00\n",
      "11321          30005370  Medicine reconciliation 2110-06-25 05:28:00\n",
      "11322          30005370  Medicine reconciliation 2110-06-25 05:28:00\n",
      "11323          30005370  Medicine reconciliation 2110-06-25 05:28:00\n",
      "11324          30005370  Medicine reconciliation 2110-06-25 05:28:00\n",
      "11325          30005370  Medicine reconciliation 2110-06-25 05:28:00\n",
      "11326          30005370  Medicine reconciliation 2110-06-25 05:28:00\n",
      "11327          30005370  Medicine reconciliation 2110-06-25 05:28:00\n",
      "11328          30005370  Medicine reconciliation 2110-06-25 05:28:00\n",
      "11329          30005370  Medicine reconciliation 2110-06-25 05:29:00\n",
      "11330          30005370  Medicine reconciliation 2110-06-25 05:29:00\n",
      "11331          30005370   Medicine dispensations 2110-06-25 05:31:00\n",
      "11332          30005370   Medicine dispensations 2110-06-25 05:31:00\n",
      "11333          30005370   Medicine dispensations 2110-06-25 05:52:00\n",
      "11334          30005370   Medicine dispensations 2110-06-25 05:52:00\n",
      "11335          30005370   Medicine dispensations 2110-06-25 05:52:00\n",
      "11336          30005370         Vital sign check 2110-06-25 06:46:00\n",
      "11337          30005370         Vital sign check 2110-06-25 08:27:00\n",
      "11338          30005370   Medicine dispensations 2110-06-25 08:31:00\n",
      "11339          30005370    Discharge from the ED 2110-06-25 09:42:00\n",
      "\n",
      "Case ID: 30011087\n",
      "      case:concept:name            concept:name      time:timestamp\n",
      "16374          30011087            Enter the ED 2110-09-06 02:51:00\n",
      "16375          30011087        Triage in the ED 2110-09-06 02:51:01\n",
      "16376          30011087  Medicine dispensations 2110-09-06 03:04:00\n",
      "16377          30011087  Medicine dispensations 2110-09-06 03:04:00\n",
      "16378          30011087  Medicine dispensations 2110-09-06 03:10:00\n",
      "16379          30011087  Medicine dispensations 2110-09-06 03:10:00\n",
      "16380          30011087  Medicine dispensations 2110-09-06 03:10:00\n",
      "16381          30011087  Medicine dispensations 2110-09-06 03:10:00\n",
      "16382          30011087  Medicine dispensations 2110-09-06 03:10:00\n",
      "16383          30011087  Medicine dispensations 2110-09-06 03:10:00\n",
      "16384          30011087  Medicine dispensations 2110-09-06 03:17:00\n",
      "16385          30011087  Medicine dispensations 2110-09-06 03:27:00\n",
      "16386          30011087  Medicine dispensations 2110-09-06 03:27:00\n",
      "16387          30011087   Discharge from the ED 2110-09-06 05:09:56\n",
      "16388          30011087   Discharge from the ED 2110-09-06 05:09:56\n",
      "16389          30011087   Discharge from the ED 2110-09-06 05:09:56\n",
      "16390          30011087   Discharge from the ED 2110-09-06 05:09:56\n",
      "\n",
      "Case ID: 30016618\n",
      "     case:concept:name            concept:name      time:timestamp\n",
      "6661          30016618            Enter the ED 2110-04-26 00:33:00\n",
      "6662          30016618        Triage in the ED 2110-04-26 00:33:01\n",
      "6663          30016618        Vital sign check 2110-04-26 00:35:00\n",
      "6664          30016618        Vital sign check 2110-04-26 03:51:00\n",
      "6665          30016618  Medicine dispensations 2110-04-26 05:07:00\n",
      "6666          30016618        Vital sign check 2110-04-26 05:36:00\n",
      "6667          30016618  Medicine dispensations 2110-04-26 06:04:00\n",
      "6668          30016618   Discharge from the ED 2110-04-26 06:18:00\n",
      "6669          30016618   Discharge from the ED 2110-04-26 06:18:00\n",
      "6670          30016618   Discharge from the ED 2110-04-26 06:18:00\n",
      "6671          30016618   Discharge from the ED 2110-04-26 06:18:00\n",
      "\n",
      "Case ID: 30022872\n",
      "      case:concept:name             concept:name      time:timestamp\n",
      "15955          30022872             Enter the ED 2110-09-01 02:50:00\n",
      "15956          30022872         Triage in the ED 2110-09-01 02:50:01\n",
      "15957          30022872  Medicine reconciliation 2110-09-01 03:03:00\n",
      "15958          30022872  Medicine reconciliation 2110-09-01 03:03:00\n",
      "15959          30022872  Medicine reconciliation 2110-09-01 03:03:00\n",
      "15960          30022872  Medicine reconciliation 2110-09-01 03:03:00\n",
      "15961          30022872  Medicine reconciliation 2110-09-01 03:03:00\n",
      "15962          30022872  Medicine reconciliation 2110-09-01 03:03:00\n",
      "15963          30022872  Medicine reconciliation 2110-09-01 03:03:00\n",
      "15964          30022872   Medicine dispensations 2110-09-01 03:20:00\n",
      "15965          30022872         Vital sign check 2110-09-01 04:15:00\n",
      "15966          30022872         Vital sign check 2110-09-01 04:40:00\n",
      "15967          30022872   Medicine dispensations 2110-09-01 04:45:00\n",
      "15968          30022872         Vital sign check 2110-09-01 04:48:00\n",
      "15969          30022872   Medicine dispensations 2110-09-01 04:56:00\n",
      "15970          30022872         Vital sign check 2110-09-01 05:29:00\n",
      "15971          30022872         Vital sign check 2110-09-01 06:02:00\n",
      "15972          30022872         Vital sign check 2110-09-01 09:30:00\n",
      "15973          30022872    Discharge from the ED 2110-09-01 10:30:00\n",
      "15974          30022872    Discharge from the ED 2110-09-01 10:30:00\n",
      "15975          30022872    Discharge from the ED 2110-09-01 10:30:00\n",
      "\n",
      "Case ID: 30023012\n",
      "      case:concept:name             concept:name      time:timestamp\n",
      "22116          30023012             Enter the ED 2110-11-25 14:52:00\n",
      "22117          30023012         Triage in the ED 2110-11-25 14:52:01\n",
      "22118          30023012         Vital sign check 2110-11-25 14:55:00\n",
      "22119          30023012  Medicine reconciliation 2110-11-25 15:43:00\n",
      "22120          30023012  Medicine reconciliation 2110-11-25 15:43:00\n",
      "22121          30023012  Medicine reconciliation 2110-11-25 15:43:00\n",
      "22122          30023012  Medicine reconciliation 2110-11-25 15:43:00\n",
      "22123          30023012  Medicine reconciliation 2110-11-25 15:43:00\n",
      "22124          30023012  Medicine reconciliation 2110-11-25 15:43:00\n",
      "22125          30023012  Medicine reconciliation 2110-11-25 15:43:00\n",
      "22126          30023012  Medicine reconciliation 2110-11-25 15:43:00\n",
      "22127          30023012  Medicine reconciliation 2110-11-25 15:43:00\n",
      "22128          30023012  Medicine reconciliation 2110-11-25 15:43:00\n",
      "22129          30023012         Vital sign check 2110-11-25 18:47:00\n",
      "22130          30023012    Discharge from the ED 2110-11-25 20:38:00\n",
      "22131          30023012    Discharge from the ED 2110-11-25 20:38:00\n",
      "\n",
      "Case ID: 30028898\n",
      "     case:concept:name             concept:name      time:timestamp\n",
      "5346          30028898             Enter the ED 2110-04-08 20:22:00\n",
      "5347          30028898         Triage in the ED 2110-04-08 20:22:01\n",
      "5348          30028898         Vital sign check 2110-04-08 20:25:00\n",
      "5349          30028898   Medicine dispensations 2110-04-08 21:35:00\n",
      "5350          30028898   Medicine dispensations 2110-04-08 21:35:00\n",
      "5351          30028898   Medicine dispensations 2110-04-08 21:35:00\n",
      "5352          30028898   Medicine dispensations 2110-04-08 21:46:00\n",
      "5353          30028898   Medicine dispensations 2110-04-08 21:48:00\n",
      "5354          30028898   Medicine dispensations 2110-04-08 21:49:00\n",
      "5355          30028898   Medicine dispensations 2110-04-08 21:49:00\n",
      "5356          30028898  Medicine reconciliation 2110-04-08 22:08:00\n",
      "5357          30028898  Medicine reconciliation 2110-04-08 22:08:00\n",
      "5358          30028898  Medicine reconciliation 2110-04-08 22:08:00\n",
      "5359          30028898         Vital sign check 2110-04-08 22:18:00\n",
      "5360          30028898    Discharge from the ED 2110-04-09 00:18:00\n",
      "5361          30028898    Discharge from the ED 2110-04-09 00:18:00\n",
      "\n",
      "Case ID: 30029204\n",
      "      case:concept:name            concept:name      time:timestamp\n",
      "19980          30029204            Enter the ED 2110-10-28 06:13:00\n",
      "19981          30029204        Triage in the ED 2110-10-28 06:13:01\n",
      "19982          30029204        Vital sign check 2110-10-28 06:14:00\n",
      "19983          30029204  Medicine dispensations 2110-10-28 06:18:00\n",
      "19984          30029204  Medicine dispensations 2110-10-28 06:18:00\n",
      "19985          30029204  Medicine dispensations 2110-10-28 06:26:00\n",
      "19986          30029204  Medicine dispensations 2110-10-28 06:27:00\n",
      "19987          30029204  Medicine dispensations 2110-10-28 06:27:00\n",
      "19988          30029204  Medicine dispensations 2110-10-28 06:27:00\n",
      "19989          30029204  Medicine dispensations 2110-10-28 06:39:00\n",
      "19990          30029204   Discharge from the ED 2110-10-28 08:41:00\n",
      "19991          30029204   Discharge from the ED 2110-10-28 08:41:00\n",
      "\n",
      "Case ID: 30041254\n",
      "      case:concept:name             concept:name      time:timestamp\n",
      "17937          30041254             Enter the ED 2110-09-28 10:55:00\n",
      "17938          30041254         Vital sign check 2110-09-28 10:55:00\n",
      "17939          30041254         Triage in the ED 2110-09-28 10:55:01\n",
      "17940          30041254  Medicine reconciliation 2110-09-28 11:31:00\n",
      "17941          30041254  Medicine reconciliation 2110-09-28 11:31:00\n",
      "17942          30041254  Medicine reconciliation 2110-09-28 11:31:00\n",
      "17943          30041254  Medicine reconciliation 2110-09-28 11:31:00\n",
      "17944          30041254  Medicine reconciliation 2110-09-28 11:31:00\n",
      "17945          30041254  Medicine reconciliation 2110-09-28 11:31:00\n",
      "17946          30041254  Medicine reconciliation 2110-09-28 11:31:00\n",
      "17947          30041254  Medicine reconciliation 2110-09-28 11:31:00\n",
      "17948          30041254  Medicine reconciliation 2110-09-28 11:31:00\n",
      "17949          30041254  Medicine reconciliation 2110-09-28 11:40:00\n",
      "17950          30041254  Medicine reconciliation 2110-09-28 11:40:00\n",
      "17951          30041254  Medicine reconciliation 2110-09-28 11:41:00\n",
      "17952          30041254   Medicine dispensations 2110-09-28 12:57:00\n",
      "17953          30041254         Vital sign check 2110-09-28 14:00:00\n",
      "17954          30041254   Medicine dispensations 2110-09-28 14:39:00\n",
      "17955          30041254         Vital sign check 2110-09-28 16:41:00\n",
      "17956          30041254    Discharge from the ED 2110-09-28 16:45:00\n",
      "\n",
      "Case ID: 30044664\n",
      "     case:concept:name           concept:name      time:timestamp\n",
      "2787          30044664           Enter the ED 2110-02-26 10:58:00\n",
      "2788          30044664       Triage in the ED 2110-02-26 10:58:01\n",
      "2789          30044664       Vital sign check 2110-02-26 11:00:00\n",
      "2790          30044664  Discharge from the ED 2110-02-26 19:00:00\n",
      "\n",
      "Case ID: 30047244\n",
      "      case:concept:name             concept:name      time:timestamp\n",
      "14538          30047244             Enter the ED 2110-08-05 00:56:00\n",
      "14539          30047244         Triage in the ED 2110-08-05 00:56:01\n",
      "14540          30047244         Vital sign check 2110-08-05 00:57:00\n",
      "14541          30047244  Medicine reconciliation 2110-08-05 02:12:00\n",
      "14542          30047244  Medicine reconciliation 2110-08-05 02:13:00\n",
      "14543          30047244  Medicine reconciliation 2110-08-05 02:13:00\n",
      "14544          30047244  Medicine reconciliation 2110-08-05 02:13:00\n",
      "14545          30047244  Medicine reconciliation 2110-08-05 02:13:00\n",
      "14546          30047244         Vital sign check 2110-08-05 03:47:00\n",
      "14547          30047244    Discharge from the ED 2110-08-05 04:20:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Attribute arrival_transport contains values of different dtypes ({Float64, String})\n",
      "Warning: Attribute gender contains values of different dtypes ({Float64, String})\n",
      "Warning: Attribute race contains values of different dtypes ({Float64, String})\n",
      "Warning: Attribute icd_title contains values of different dtypes ({String, Float64})\n",
      "Warning: Attribute icd_code contains values of different dtypes ({String, Float64})\n",
      "Warning: Attribute disposition contains values of different dtypes ({Float64, String})\n",
      "Warning: Attribute pain contains values of different dtypes ({Float64, String})\n",
      "Warning: Attribute rhythm contains values of different dtypes ({String, Float64})\n",
      "Warning: Attribute etcdescription contains values of different dtypes ({Float64, String})\n",
      "Warning: Attribute name contains values of different dtypes ({String, Float64})\n"
     ]
    }
   ],
   "source": [
    "import pm4py\n",
    "from log_processing import convert_to_datetime\n",
    "import pandas as pd\n",
    "\n",
    "log_dir = \"data\"\n",
    "folder_name = \"mimicel_mini_2000\"\n",
    "\n",
    "log_file = f\"{folder_name}.xes\"\n",
    "log_df = pm4py.read_xes(os.path.join(log_dir, log_file))\n",
    "log_df = convert_to_datetime(log_df, \"time:timestamp\")\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(f\"Columns: {log_df.columns.tolist()}\")\n",
    "activities = log_df['concept:name'].unique()\n",
    "print(f\"Activities: {sorted(activities)}\")\n",
    "print(f\"Number of unique activities: {len(activities)}\")\n",
    "print(f\"Cases: {len(log_df['case:concept:name'].unique())}\")\n",
    "print(f\"Total number of events: {len(log_df)}\")\n",
    "print(f\"Average number of events per case: {log_df.groupby('case:concept:name').size().mean()}\")\n",
    "print(f\"Average event duration: {log_df['time:timestamp'].diff().mean()}\")\n",
    "\n",
    "\n",
    "log_df = log_df[[\"case:concept:name\", \"concept:name\", \"time:timestamp\"]]\n",
    "log_df_sorted = log_df.sort_values(by=['case:concept:name', \"time:timestamp\"])\n",
    "print(\"Event log head:\")\n",
    "print(log_df_sorted.head(100))\n",
    "\n",
    "# group by case and print first few cases\n",
    "grouped = log_df_sorted.groupby('case:concept:name')\n",
    "for case_id, group in list(grouped)[:10]:\n",
    "    print(f\"\\nCase ID: {case_id}\")\n",
    "    print(group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770181ce",
   "metadata": {},
   "source": [
    "# Old Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35af2d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare...\n",
      "Done.\n",
      "Load event log...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tensorflow/python/client/session.py:1793: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ad777ca3d954f7bbdd9ce0d6ce6bf96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "parsing log, completed traces ::   0%|          | 0/15001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Create log statistics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Extract DataFrame for evaluation...\n",
      "First 10 prefixes:\n",
      "[[{'lifecycle:transition': 'complete', 'time:timestamp': datetime.datetime(2015, 7, 1, 8, 30, 31, tzinfo=datetime.timezone.utc), '(case)_creator': 'Fluxicon Disco', '(case)_variant': 'Variant 261', '(case)_variant-index': '261', 'age': '18-29', 'concept:name': 'question', 'customer_id': '2046012', 'gender': 'V', 'office_u': '296', 'office_w': '269', 'question': 'Application/Registration WW'}]]\n",
      "      caseID  prefix-length     true-next-activity  true-next-time\n",
      "0   18684438              1               question        0.002662\n",
      "1   18684438              2           aanvragen-ww        0.060613\n",
      "2   18684438              3                   home        0.030463\n",
      "3   18684438              4           aanvragen-ww        0.000243\n",
      "4   18684438              5                   home        0.008553\n",
      "5   18684438              6                    END        0.000000\n",
      "6    9706911              1                  taken        0.570220\n",
      "7    9706911              2  vacatures_bij_mijn_cv        0.000266\n",
      "8    9706911              3                    END        0.000000\n",
      "9   28335508              1           mijn_werkmap        0.571424\n",
      "10  28335508              2                  taken        0.000741\n",
      "11  28335508              3  vacatures_bij_mijn_cv        0.000475\n",
      "12  28335508              4                   home        0.001771\n",
      "13  28335508              5                    END        0.000000\n",
      "14   5623699              1               question        0.000081\n",
      "15   5623699              2         mijn_berichten        0.003137\n",
      "16   5623699              3                    END        0.000000\n",
      "17  28589554              1               question        0.006088\n",
      "18  28589554              2                   home        1.075914\n",
      "19  28589554              3                  taken        0.000532\n",
      "Evaluate prediction models...\n",
      "This might take a while...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 21 is out of bounds for axis 0 with size 21",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[38], line 221\u001b[0m\n",
      "\u001b[1;32m    218\u001b[0m                         log_encoder\u001b[38;5;241m.\u001b[39mfit(log, activities\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mactivities, data_attributes\u001b[38;5;241m=\u001b[39mdata_attributes,\n",
      "\u001b[1;32m    219\u001b[0m                                         text_attribute\u001b[38;5;241m=\u001b[39mtext_attribute)\n",
      "\u001b[1;32m    220\u001b[0m                         \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iterations):\n",
      "\u001b[0;32m--> 221\u001b[0m                             \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_log\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_attributes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_attributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_attribute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_attribute\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    222\u001b[0m                             model\u001b[38;5;241m.\u001b[39mevaluate(test_log, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_prefixes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n",
      "\u001b[1;32m    223\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone. Evaluation completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "File \u001b[0;32m~/reddit-mining/src/tapp/tapp_model.py:83\u001b[0m, in \u001b[0;36mTappModel.fit\u001b[0;34m(self, log, data_attributes, text_attribute, epochs, validation_split, patience_rl, patience_es, batch_size)\u001b[0m\n",
      "\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, log, data_attributes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, text_attribute\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, patience_rl\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n",
      "\u001b[1;32m     79\u001b[0m         patience_es\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# Encode training data\u001b[39;00m\n",
      "\u001b[1;32m     81\u001b[0m     \u001b[38;5;66;03m# self.activities = _get_event_labels(log, \"concept:name\")\u001b[39;00m\n",
      "\u001b[1;32m     82\u001b[0m     \u001b[38;5;66;03m# self.log_encoder.fit(log, activities=self.activities, data_attributes=data_attributes, text_attribute=text_attribute)\u001b[39;00m\n",
      "\u001b[0;32m---> 83\u001b[0m     x, y_next_act, y_next_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfor_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     84\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber of training cases:\u001b[39m\u001b[38;5;124m\"\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;32m     86\u001b[0m     \u001b[38;5;66;03m# Build model\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/reddit-mining/src/tapp/log_encoder.py:194\u001b[0m, in \u001b[0;36mLogEncoder.transform\u001b[0;34m(self, log, for_training)\u001b[0m\n",
      "\u001b[1;32m    192\u001b[0m     x[trace_dim_index][padding\u001b[38;5;241m+\u001b[39mevent_index][offset \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m=\u001b[39m (event_time\u001b[38;5;241m.\u001b[39mhour \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m3600\u001b[39m \u001b[38;5;241m+\u001b[39m event_time\u001b[38;5;241m.\u001b[39msecond)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_scaling_divisor[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# Seconds since last Monday\u001b[39;00m\n",
      "\u001b[0;32m--> 194\u001b[0m     \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrace_dim_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mevent_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m (event_time\u001b[38;5;241m.\u001b[39mweekday() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m86400\u001b[39m \u001b[38;5;241m+\u001b[39m event_time\u001b[38;5;241m.\u001b[39mhour \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m3600\u001b[39m \u001b[38;5;241m+\u001b[39m event_time\u001b[38;5;241m.\u001b[39msecond)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_scaling_divisor[\u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[1;32m    195\u001b[0m     offset \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n",
      "\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\n",
      "\u001b[0;31mIndexError\u001b[0m: index 21 is out of bounds for axis 0 with size 21"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "def extract_df(log):\n",
    "    # Make predictions\n",
    "    prefix_log = [case[0:prefix_length] for case in log for prefix_length in range(1, len(case) + 1)]\n",
    "    print(\"First 10 prefixes:\")\n",
    "    print(prefix_log[:1])\n",
    "    caseIDs = []\n",
    "    prefix_lengths = []\n",
    "    true_next_activities = []\n",
    "    true_next_times = []\n",
    "    for case in log:\n",
    "        caseID = case.attributes[\"concept:name\"]\n",
    "        for prefix_length in range(1, len(case) + 1):\n",
    "            caseIDs.append(caseID)\n",
    "            prefix_lengths.append(prefix_length)\n",
    "            true_next_activities.append(\"END\" if prefix_length == len(case) else case[prefix_length][\"concept:name\"])\n",
    "            true_next_times.append(0 if prefix_length == len(case) else (case[prefix_length][\"time:timestamp\"].timestamp() -\n",
    "                                                                            case[prefix_length - 1][\"time:timestamp\"].timestamp()) / 86400)\n",
    "\n",
    "    # Generate DataFrame\n",
    "    column_data = {\"caseID\": caseIDs, \"prefix-length\": prefix_lengths, \"true-next-activity\": true_next_activities,\n",
    "        \"true-next-time\": true_next_times,}\n",
    "    columns = [\"caseID\", \"prefix-length\", \"true-next-activity\", \"true-next-time\"]\n",
    "    return pd.DataFrame(column_data, columns=columns)\n",
    "\n",
    "print(\"Prepare...\")\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "from tapp.tapp_model import TappModel, _get_event_labels\n",
    "from tapp.log_encoder import LogEncoder\n",
    "from tapp.text_encoder import BoWTextEncoder\n",
    "from tapp.text_encoder import BoNGTextEncoder\n",
    "from tapp.text_encoder import PVTextEncoder\n",
    "from tapp.text_encoder import LDATextEncoder\n",
    "from tapp.text_encoder import BERTbaseTextEncoder\n",
    "from tapp.text_encoder import BERTbaseFineTunedNextActivityTextEncoder\n",
    "from tapp.text_encoder import BERTbaseFineTunedNextTimeTextEncoder\n",
    "from tapp.text_encoder import BERTbaseFineTunedNextActivityAndTimeTextEncoder\n",
    "from tapp.text_encoder import BERTfromScratchTextEncoder\n",
    "from tapp.text_encoder import BERTAndTokenizerFromScratchTextEncoder\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pm4py.algo.filtering.log.variants import variants_filter\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "\n",
    "# Workstation\n",
    "from tensorflow.core.protobuf.config_pb2 import ConfigProto\n",
    "from tensorflow.python.client.session import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "# Download text preprocessing resources from nltk\n",
    "try:\n",
    "    nltk.data.find(\"corpora/wordnet\")\n",
    "except LookupError:\n",
    "    nltk.download(\"wordnet\")\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except LookupError:\n",
    "    nltk.download(\"punkt\")\n",
    "try:\n",
    "    nltk.data.find(\"corpora/stopwords\")\n",
    "except LookupError:\n",
    "    nltk.download(\"stopwords\")\n",
    "\n",
    "if not os.path.exists('./results/'):\n",
    "    os.makedirs('./results/')\n",
    "\n",
    "print(\"Done.\")\n",
    "\n",
    "# Load event data\n",
    "print(\"Load event log...\")\n",
    "path = \"./data/werk.xes\"\n",
    "variant = xes_importer.Variants.ITERPARSE\n",
    "parameters = {variant.value.Parameters.TIMESTAMP_SORT: True, variant.value.Parameters.REVERSE_SORT: False}\n",
    "log = xes_importer.apply(path, variant=variant, parameters=parameters)\n",
    "print(\"Done.\")\n",
    "\n",
    "\n",
    "# Analyse log\n",
    "print(\"Create log statistics...\")\n",
    "language = \"english\" \n",
    "text_attribute = \"question\"\n",
    "traces = len(log)\n",
    "events = sum(len(case) for case in log)\n",
    "durations = [(case[-1][\"time:timestamp\"].timestamp() - case[0][\"time:timestamp\"].timestamp()) / 86400 for case in log]\n",
    "docs = [event[text_attribute] for case in log for event in case if text_attribute in event]\n",
    "words = [word for doc in docs for word in word_tokenize(doc, language=language)]\n",
    "docs_filtered = BoWTextEncoder().preprocess_docs(docs, as_list=False)\n",
    "words_filtered = [word for doc in docs_filtered for word in word_tokenize(doc, language=language)]\n",
    "\n",
    "log_info = pd.DataFrame(\n",
    "    [[path,\n",
    "      traces,\n",
    "      len(variants_filter.get_variants(log)),\n",
    "      events,\n",
    "      events / traces,\n",
    "      np.median(durations),\n",
    "      np.mean(durations),\n",
    "      len(list(dict.fromkeys([event[\"concept:name\"] for case in log for event in case])) if log else []),\n",
    "      len(words),\n",
    "      len(words_filtered),\n",
    "      len(set(words)),\n",
    "      len(set(words_filtered))]],\n",
    "    columns=[\"log\", \"cases\", \"trace variants\", \"events\", \"events per trace\", \"median case duration\",\n",
    "             \"mean case duration\", \"activities\", \"words pre filtering\", \"words post filtering\",\n",
    "             \"vocabulary pre filtering\", \"vocabulary post filtering\"]\n",
    ")\n",
    "\n",
    "log_info.to_csv(\"./results/log_info.csv\", index=False, sep=\";\")\n",
    "print(\"Done.\")\n",
    "\n",
    "# Split data in train and test log\n",
    "split = len(log) // 5 * 4\n",
    "train_log = log[:split]\n",
    "test_log = log[split:]\n",
    "\n",
    "# Configure and build model variants\n",
    "language = \"english\"\n",
    "\n",
    "# configure text base for training:\n",
    "# 'event' -> treat text attributes as event attributes\n",
    "# 'prefix' -> use concatenation of text attributes from events\n",
    "text_base_for_training = 'event'\n",
    "\n",
    "text_models = [\n",
    "    # --- Baselines from Pegoraro et al. ---\n",
    "    None,\n",
    "    BoWTextEncoder(encoding_length=50, language=language),\n",
    "    BoWTextEncoder(encoding_length=100, language=language),\n",
    "    BoWTextEncoder(encoding_length=500, language=language),\n",
    "    BoNGTextEncoder(n=2, encoding_length=50, language=language),\n",
    "    BoNGTextEncoder(n=2, encoding_length=100, language=language),\n",
    "    BoNGTextEncoder(n=2, encoding_length=500, language=language),\n",
    "    PVTextEncoder(encoding_length=10, language=language),\n",
    "    PVTextEncoder(encoding_length=20, language=language),\n",
    "    PVTextEncoder(encoding_length=100, language=language),\n",
    "    LDATextEncoder(encoding_length=10, language=language),\n",
    "    LDATextEncoder(encoding_length=20, language=language),\n",
    "    LDATextEncoder(encoding_length=100, language=language),\n",
    "\n",
    "    # --- TAPPBERT ---\n",
    "    # Pre-trained BERT\n",
    "    BERTbaseTextEncoder(encoding_length=768, language=language),\n",
    "    # Pre-trained + fine-tuned BERT\n",
    "    # (1) fine-tuned toward next activity prediction\n",
    "    BERTbaseFineTunedNextActivityTextEncoder(encoding_length=768, language=language, epochs=16, lr=5e-5),\n",
    "    # (2) fine-tuned toward next timestamp prediction\n",
    "    BERTbaseFineTunedNextTimeTextEncoder(encoding_length=768, language=language, epochs=16, lr=5e-5),\n",
    "    # (3) concat. embeddings of BERT fine-tuned toward next activity + next timestamp prediction\n",
    "    BERTbaseFineTunedNextActivityAndTimeTextEncoder(encoding_length=768, language=language, epochs=16, lr=5e-5),\n",
    "    # BERT trained from scratch\n",
    "    # (1) tokenizer is pre-trained\n",
    "    BERTfromScratchTextEncoder(encoding_length=36, language=language),\n",
    "    BERTfromScratchTextEncoder(encoding_length=768, language=language),\n",
    "    # (2) tokenizer is trained from scratch\n",
    "    BERTAndTokenizerFromScratchTextEncoder(encoding_length=36, language=language, vocab_size=1000),\n",
    "    BERTAndTokenizerFromScratchTextEncoder(encoding_length=768, language=language, vocab_size=1000),\n",
    "]\n",
    "\n",
    "text_models = [\n",
    "    BERTbaseFineTunedNextActivityTextEncoder(encoding_length=768, language=language, epochs=16, lr=5e-5),\n",
    "]\n",
    "\n",
    "if BERTfromScratchTextEncoder in text_models or BERTAndTokenizerFromScratchTextEncoder in text_models:\n",
    "    text_data_path = \"../../datasets/questions.txt\"\n",
    "    if not os.path.exists(text_data_path):\n",
    "        # extract text and store in separate file to be used during pretraining BERT from scratch\n",
    "        df = log_converter.apply(log, variant=log_converter.Variants.TO_DATA_FRAME)\n",
    "        txt = df[\"question\"].dropna().values.tolist()\n",
    "        with open(text_data_path, \"w\", encoding=\"utf-8\") as output:\n",
    "            for doc in txt:\n",
    "                sentences = nltk.tokenize.sent_tokenize(doc)\n",
    "                for sentence in sentences:\n",
    "                    output.write(sentence)\n",
    "                    output.write('\\n')\n",
    "                output.write('\\n')\n",
    "\n",
    "\n",
    "shared_layers = [1]\n",
    "special_layers = [1]\n",
    "neurons = [100]\n",
    "data_attributes_list = [[]]\n",
    "iterations = 1\n",
    "\n",
    "print(\"Extract DataFrame for evaluation...\")\n",
    "df = extract_df(log)\n",
    "print(df.head(20))\n",
    "\n",
    "print(\"Evaluate prediction models...\")\n",
    "print(\"This might take a while...\")\n",
    "for text_model in text_models:\n",
    "    for shared_layer in shared_layers:\n",
    "        for special_layer in special_layers:\n",
    "            for neuron in neurons:\n",
    "                for data_attributes in data_attributes_list:\n",
    "                    if shared_layer + special_layer == 0:\n",
    "                        pass\n",
    "                    else:\n",
    "                        log_encoder = LogEncoder(text_encoder=text_model, advanced_time_attributes=True,\n",
    "                                                 text_base_for_training=text_base_for_training)\n",
    "                        model = TappModel(log_encoder=log_encoder, num_shared_layer=shared_layer,\n",
    "                                          num_specialized_layer=special_layer, neurons_per_layer=neuron, dropout=0.2,\n",
    "                                          learning_rate=0.001)\n",
    "                        model.activities = _get_event_labels(log, \"concept:name\")\n",
    "                        log_encoder.fit(log, activities=model.activities, data_attributes=data_attributes,\n",
    "                                        text_attribute=text_attribute)\n",
    "                        for iteration in range(iterations):\n",
    "                            model.fit(train_log, data_attributes=data_attributes, text_attribute=text_attribute, epochs=25)\n",
    "                            model.evaluate(test_log, \"results.csv\", num_prefixes=8)\n",
    "print(\"Done. Evaluation completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
